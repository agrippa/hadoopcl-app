0. ParseWiki (only runs for wiki because wiki is in XML format)

Uses an XML parser to iterate through the documents in the wiki data set and
dump the bodies of the documents to a collection of text files.

1. WikiTxtToSeq / ASFTxtToSeq

Converts a collection of text files (I believe those generated by ParseWiki in
the case of the wiki data set) to sequence files where the key is a file name
and the value is the body of that file. Multi-threaded.

2. TextToTokens

Takes the output of WikiTxtToSeq/ASFTxtToSeq (filename paired with String body)
and converts to sequence files where the value is just a tokenized body instead
(with type StringTuple). Also multi-threaded.

3. WordCountFromTokenized

Iterates over seq files produced by TextToTokens. It divides the tokenized files
into smaller chunks (10 chunks is hard-coded at the moment) and processes these
chunks one-by-one, but uses threads to process each chunk. For each (filename,
tokenized file body) pair inside those files it stores a count of all the tokens.
It aggregates these counts across all files in a chunk and finally writes an
output sequence file for the current chunk containing pairs of (token/word, total
count of that word). It does this for each chunk, so you're left with 10 seq
files which each contain the total count of each word for that chunk. Why was it
done this way? Pretty sure it was done because of memory constraints.

4. MergeWordCount

MergeWordCount takes the multiple count files from WordCountFromTokenized and
merges their results into a single, global sequence file which contains for each
word/token in the original text files a global count of how many instances of
that word there are in the whole corpus.

5. GenerateUniqueWordIDs

Creates unique integer IDs for each word discovered in WordCountFromTokenized
and MergeWordCount, and output pairs of (word, integer ID) to a sequence file.
These mappings are used to later produce numeric vectors representing text
documents.

6. GenerateDFVectors

This does a lot of stuff. First, it uses a readDictionary function to store an
in-memory map from work/token to its unique integer ID based on the seq file
output by GenerateUniqueWordIDs. It then iterates over the tokenized documents
from TextToTokens and for each tokenized document contained in those seq files,
it uses processInputPair to convert the StringTuple to a VectorWritable.
processInputPair just iterates over the Strings in the StringTuple, gets that
String's unique ID from the word map, and increments the entry in a sparse vector
associated with that integer ID. That accumulated vector for the StringTuple is
returned. Pairs of (filename, feature vector) are written to output sequence
files.

7. TFtoDF

TFtoDF starts the same as GenerateDFVectors by reading the word count dictionary
into memory. It then produces an output sequence file based on this word count
map and the total per-word count across the corpus generated in
WordCountFromTokenized/MergeWordCount. This output sequence file contains total
feature counts across the corpus, so it is pairings of (unique token/feature ID,
total count). TFtoDF then just prints to STDOUT the total number of vectors (i.e.
documents) in the data set and the total number of features (i.e. words/tokens).

8. PruneTF

Starts by reading the (feature ID, global count) pairs from TFtoDF into an
in-memory map. Then iterates over the (filename, feature vector) pairs produced
by GenerateDFVectors and for each feature vector produces a transformed version
which removes any elements in the feature vectors whose value is > maxDF or
< minDF. At the moment, minDF == 1 and maxDF == (total # documents * 0.85). It
then writes the new pair of (filename, pruned vector) out.

9. TFPrunedToTFIDF

Final stage of the pipeline, taking the pruned vectors from PruneTF and the
global feature counts from TFtoDF. This just translates the pruned vectors to
different "TFIDF" values using some functions from Mahout.

10. MyPruner

This pruner is not part of the standard Mahout pipeline, and was added by me
(Max) when I found that the reducers in Mahout were outputting massive sparse
vectors (hundreds of millions of elements, or ~0.5GB per vector). This is
probably due to an error in my port of the pipeline, but rather than hunt down
the error this function supports basic pruning of the outputs produced by
TFPrunedToTFIDF based on percentiles, which allows us to play with how large the
vectors we produce are.
