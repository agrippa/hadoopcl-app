jmg3      1924  0.0  0.0 106084  1396 ?        Ss   15:24   0:00 bash -c ps aux | grep java
jmg3      1940  0.0  0.0 103232   836 ?        S    15:24   0:00 grep java
jmg3      8298  0.0  0.0  59072  3528 pts/0    S+   15:24   0:00 ssh -o ConnectTimeout=2 gpu-010 ps aux | grep java
jmg3      8304  0.0  0.0 106084  1400 ?        Ss   15:24   0:00 bash -c ps aux | grep java
jmg3      8320  0.0  0.0 103232   844 ?        S    15:24   0:00 grep java
java: no process killed
java: no process killed
jmg3      1987  0.0  0.0 106084  1404 ?        Ss   15:24   0:00 bash -c ps aux | grep java
jmg3      2005  0.0  0.0 103232   840 ?        S    15:24   0:00 grep java
jmg3      8382  0.0  0.0  59072  3532 pts/0    S+   15:24   0:00 ssh -o ConnectTimeout=2 gpu-010 ps aux | grep java
jmg3      8388  0.0  0.0 106084  1404 ?        Ss   15:24   0:00 bash -c ps aux | grep java
jmg3      8404  0.0  0.0 103232   840 ?        S    15:24   0:00 grep java
Setting path to /tmp/yiskylee
12 2 0 0 0 0 67108864 16
Max. num tasks = 12 mappers, 2 reducers
Mapper thread configs = GPU ( 0 ), CPU ( 0 )
Reducer thread configs = GPU ( 0 ), CPU ( 0 )
HDFS block size = 67108864
GPU/CPU Mults shouldn't matter



Here are the results:
export JAVA_HOME=/opt/apps/jdk/1.6
export HOME=/home/jmg3
export HADOOP_HOME=${HOME}/hadoop-1.0.3
export APARAPI_HOME=${HOME}/aparapi-read-only
export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true
export HADOOPCL_KERNEL_FOLDER=${HOME}/kernels
export HADOOP_LOG_DIR=/tmp/hadoop/logs
export HADOOP_APP_DIR=${HOME}/app
export CLASSPATH=${APARAPI_HOME}/com.amd.aparapi/dist/aparapi.jar:${HADOOP_HOME}/build/hadoop-core-1.0.4-SNAPSHOT.jar:${HOME}/hadoop-gpl-compression-read-only/build/hadoop-gpl-compression-0.2.0-dev.jar:${HADOOP_HOME}/build/hadoop-core-1.0.4-SNAPSHOT.jar:${HOME}/jfreechart-1.0.14.jar:.:${CLASSPATH}:/home/jmg3/mahout/math/target/mahout-math-0.9-SNAPSHOT.jar:/home/jmg3/mahout/integration/target/dependency/mahout-core-0.9-SNAPSHOT.jar
export HADOOP_CLASSPATH=${HOME}/hadoop-gpl-compression-read-only/build/hadoop-gpl-compression-0.2.0-dev.jar:${CLASSPATH}:${HADOOP_CLASSPATH}:/home/jmg3/mahout/math/target/mahout-math-0.9-SNAPSHOT.jar:/home/jmg3/mahout/integration/target/dependency/mahout-core-0.9-SNAPSHOT.jar
export JAVA_LIBRARY_PATH=${HOME}/lzo-install/lib:${JAVA_LIBRARY_PATH}

export SCIPY_HOME=${HOME}/scipy-install
export NUMPY_HOME=${HOME}/numpy-install
export SCIKIT_HOME=${HOME}/scikit-install
export PYTHONPATH=${SCIKIT_HOME}/lib64/python2.6/site-packages:${NUMPY_HOME}/lib64/python2.6/site-packages:${SCIPY_HOME}/lib64/python2.6/site-packages:${PYTHONPATH}

-----------------------------------------------------
  <name>mapred.job.tracker</name>
  <value>gpu-010.davinci.rice.edu:54311</value>
  <name>mapred.task.tracker.http.address</name>
  <value>0.0.0.0:50061</value>
  <name>mapred.reduce.parallel.copies</name><value>5</value>
  <name>tasktracker.http.threads</name><value>40</value>
  <name>mapred.map.tasks</name><value>24</value>
  <name>mapred.reduce.tasks</name><value>2</value>
  <name>mapred.tasktracker.map.tasks.maximum</name><value>12</value>
  <name>mapred.tasktracker.reduce.tasks.maximum</name><value>2</value>
  <name>mapred.map.tasks.speculative.execution</name>
  <value>true</value>
  <name>mapred.task.timeout</name><value>3600000</value>
  <!--name>mapred.task.timeout</name><value>1200000</value-->
  <name>mapred.child.java.opts</name><value>-Xms2G -Xmx16G -Dopencl.mapper.threadsPerGroup.gpu=0 -Dopencl.mapper.threadsPerGroup.cpu=0 -Dopencl.reducer.threadsPerGroup.gpu=0 -Dopencl.reducer.threadsPerGroup.cpu=0 -Dopencl.scheduler=BALANCED -Dopencl.vectorsToBuffer=32768</value>
  <name>mapred.map.tasks.speculative.execution</name><value>false</value>
  <name>mapred.reduce.tasks.speculative.execution</name><value>false</value>
  <name>mapreduce.map.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLBalanceScheduler</value>
  <!--name>mapreduce.map.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLML4Scheduler</value-->
  <name>mapreduce.reduce.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLBalanceScheduler</value>
  <!--name>mapreduce.reduce.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLML4Scheduler</value-->
  <name>io.compression.codecs</name>
  <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec</value>
  <name>io.sort.spill.percent</name>
  <value>0.7</value>
  <name>io.sort.spill.percent.hadoopcl</name>
  <value>0.6</value>
  <name>io.sort.spill.percent.hadoopcl.initial</name>
  <value>0.5</value>
  <name>io.sort.quickrestart</name>
  <value>0.4</value>
  <name>io.sort.maxrestart</name>
  <value>0.6</value>
  <name>io.sort.mb</name>
  <value>100</value>
  <name>min.num.spills.for.combine</name>
  <value>3</value>
  <name>opencl.global.buckets</name>
  <value>65536</value>
  <name>opencl.spill.chunk</name>
  <value>2</value>
<property><name>opencl.mapper.nkernels</name><value>1</value></property>
<property><name>opencl.mapper.ninputbuffers</name><value>2</value></property>
<property><name>opencl.mapper.noutputbuffers</name><value>4</value></property>
<property><name>opencl.mapper.prealloc.length.int</name><value>1966080</value></property>
<property><name>opencl.mapper.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.mapper.prealloc.length.double</name><value>1966080</value></property>
<property><name>opencl.mapper.inputBufferSize</name><value>65536</value></property>
<property><name>opencl.mapper.outputBufferSize</name><value>65536</value></property>
<property><name>opencl.mapper.val_ele_multiplier</name><value>15</value></property>
<!--property><name>opencl.mapper.compare</name><value>true</value></property-->
<property><name>opencl.combiner.nkernels</name><value>1</value></property>
<property><name>opencl.combiner.ninputbuffers</name><value>1</value></property>
<property><name>opencl.combiner.noutputbuffers</name><value>1</value></property>
<property><name>opencl.combiner.prealloc.length.int</name><value>7340032</value></property>
<property><name>opencl.combiner.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.combiner.prealloc.length.double</name><value>6291456</value></property>
<property><name>opencl.combiner.inputBufferSize</name><value>6000</value></property>
<property><name>opencl.combiner.outputBufferSize</name><value>16384</value></property>
<property><name>opencl.combiner.val_multiplier</name><value>60</value></property>
<property><name>opencl.combiner.val_ele_multiplier</name><value>20</value></property>
<property><name>opencl.reducer.nkernels</name><value>1</value></property>
<property><name>opencl.reducer.ninputbuffers</name><value>1</value></property>
<property><name>opencl.reducer.noutputbuffers</name><value>1</value></property>
<property><name>opencl.reducer.prealloc.length.int</name><value>16777216</value></property>
<property><name>opencl.reducer.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.reducer.prealloc.length.double</name><value>16777216</value></property>
<property><name>opencl.reducer.inputBufferSize</name><value>65536</value></property>
<property><name>opencl.reducer.outputBufferSize</name><value>2048</value></property>
  <name>opencl.buffer.diagnostics</name>
  <value>false</value>
  <name>opencl.profiling</name>
  <value>false</value>
  <name>opencl.highlevel</name>
  <value>false</value>
  <name>opencl.recordings_folder</name>
  <value>/scratch/jmg3/hadoopcl-recordings/</value>
-----------------------------------------------------
  <name>dfs.safemode.threshold.pct</name>
  <value>0</value>
  <name>dfs.replication</name><value>3</value>
  <name>dfs.block.size</name><value>67108864</value>
  <name>dfs.datanode.handler.count</name><value>3</value>
  <name>dfs.namenode.handler.count</name><value>10</value>
  <name>dfs.datanode.max.xcievers</name><value>256</value>
-----------------------------------------------------
  <name>hadoop.tmp.dir</name>
  <value>/tmp/hadoop/hadoop-${user.name}</value>
  <name>fs.default.name</name>
  <value>hdfs://gpu-010.davinci.rice.edu:54313</value>
<name>io.compression.codecs</name>
<value>
</value>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
-----------------------------------------------------
gpu-009
-----------------------------------------------------
gpu-010
-----------------------------------------------------
Completed reconfiguring
Warning: $HADOOP_HOME is deprecated.

14/03/23 15:24:57 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gpu-010.davinci.rice.edu/192.168.110.210
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 1.0.4-SNAPSHOT
STARTUP_MSG:   build =  -r ; compiled by 'jmg3' on Sun Mar 23 13:28:23 CDT 2014
************************************************************/
14/03/23 15:24:57 INFO util.GSet: VM type       = 64-bit
14/03/23 15:24:57 INFO util.GSet: 2% max memory = 17.77875 MB
14/03/23 15:24:57 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/03/23 15:24:57 INFO util.GSet: recommended=2097152, actual=2097152
14/03/23 15:24:57 INFO namenode.FSNamesystem: fsOwner=jmg3
14/03/23 15:24:57 INFO namenode.FSNamesystem: supergroup=supergroup
14/03/23 15:24:57 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/03/23 15:24:57 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/03/23 15:24:57 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/03/23 15:24:57 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/03/23 15:24:57 INFO common.Storage: Image file of size 110 saved in 0 seconds.
14/03/23 15:24:58 INFO common.Storage: Storage directory /tmp/hadoop/hadoop-jmg3/dfs/name has been successfully formatted.
14/03/23 15:24:58 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gpu-010.davinci.rice.edu/192.168.110.210
************************************************************/
Completed namenode startup
Warning: $HADOOP_HOME is deprecated.

no jobtracker to stop
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: no tasktracker to stop
no namenode to stop
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: no datanode to stop
gpu-010: Warning: $HADOOP_HOME is deprecated.
gpu-010: 
gpu-010: no secondarynamenode to stop
Completed stop all
Warning: $HADOOP_HOME is deprecated.

starting namenode, logging to /tmp/hadoop/logs/hadoop-jmg3-namenode-gpu-010.davinci.rice.edu.out
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: starting datanode, logging to /tmp/hadoop/logs/hadoop-jmg3-datanode-gpu-009.davinci.rice.edu.out
gpu-010: Warning: $HADOOP_HOME is deprecated.
gpu-010: 
gpu-010: starting secondarynamenode, logging to /tmp/hadoop/logs/hadoop-jmg3-secondarynamenode-gpu-010.davinci.rice.edu.out
starting jobtracker, logging to /tmp/hadoop/logs/hadoop-jmg3-jobtracker-gpu-010.davinci.rice.edu.out
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: starting tasktracker, logging to /tmp/hadoop/logs/hadoop-jmg3-tasktracker-gpu-009.davinci.rice.edu.out
Completed start all
Warning: $HADOOP_HOME is deprecated.

Warning: $HADOOP_HOME is deprecated.

Warning: $HADOOP_HOME is deprecated.

MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.
Warning: $HADOOP_HOME is deprecated.

Running on hadoop, using /home/jmg3/hadoop-1.0.3/bin/hadoop and HADOOP_CONF_DIR=/home/jmg3/hadoop-1.0.3/conf
MAHOUT-JOB: /home/jmg3/mahout/examples/target/mahout-examples-0.9-SNAPSHOT-job.jar
Warning: $HADOOP_HOME is deprecated.

14/03/23 15:27:23 INFO common.AbstractJob: Command line arguments: {--clusters=[clusters], --convergenceDelta=[0.5], --distanceMeasure=[org.apache.mahout.common.distance.CosineDistanceMeasure], --endPhase=[2147483647], --input=[input], --maxIter=[1], --method=[mapreduce], --output=[output], --overwrite=null, --startPhase=[0], --tempDir=[temp]}
14/03/23 15:27:24 INFO kmeans.KMeansDriver: Input: input Clusters In: clusters Out: output Distance: org.apache.mahout.common.distance.CosineDistanceMeasure
14/03/23 15:27:24 INFO kmeans.KMeansDriver: convergence: 0.5 max Iterations: 1
14/03/23 15:27:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/03/23 15:27:24 INFO compress.CodecPool: Got brand-new decompressor
Cluster Iterator running iteration 1/1 over priorPath: output/clusters-0
Sending 0 to HDFS took 32 ms
14/03/23 15:28:04 INFO input.FileInputFormat: Total input paths to process : 100
14/03/23 15:28:04 INFO mapred.JobClient: Running job: job_201403231525_0001
14/03/23 15:28:05 INFO mapred.JobClient:  map 0% reduce 0%
14/03/23 15:28:36 INFO mapred.JobClient:  map 1% reduce 0%
14/03/23 15:28:48 INFO mapred.JobClient:  map 2% reduce 0%
14/03/23 15:29:00 INFO mapred.JobClient:  map 3% reduce 0%
14/03/23 15:29:09 INFO mapred.JobClient:  map 4% reduce 0%
14/03/23 15:29:24 INFO mapred.JobClient:  map 5% reduce 0%
14/03/23 15:29:33 INFO mapred.JobClient:  map 6% reduce 0%
14/03/23 15:29:45 INFO mapred.JobClient:  map 7% reduce 0%
14/03/23 15:29:54 INFO mapred.JobClient:  map 8% reduce 0%
14/03/23 15:30:07 INFO mapred.JobClient:  map 9% reduce 0%
14/03/23 15:30:19 INFO mapred.JobClient:  map 10% reduce 0%
14/03/23 15:30:28 INFO mapred.JobClient:  map 11% reduce 0%
14/03/23 15:30:46 INFO mapred.JobClient:  map 12% reduce 2%
14/03/23 15:30:52 INFO mapred.JobClient:  map 12% reduce 3%
14/03/23 15:31:01 INFO mapred.JobClient:  map 13% reduce 3%
14/03/23 15:31:13 INFO mapred.JobClient:  map 14% reduce 3%
14/03/23 15:31:22 INFO mapred.JobClient:  map 15% reduce 3%
14/03/23 15:31:34 INFO mapred.JobClient:  map 16% reduce 3%
14/03/23 15:31:43 INFO mapred.JobClient:  map 17% reduce 3%
14/03/23 15:31:55 INFO mapred.JobClient:  map 18% reduce 3%
14/03/23 15:32:07 INFO mapred.JobClient:  map 19% reduce 3%
14/03/23 15:32:16 INFO mapred.JobClient:  map 20% reduce 3%
14/03/23 15:32:28 INFO mapred.JobClient:  map 21% reduce 3%
14/03/23 15:32:39 INFO mapred.JobClient:  map 22% reduce 3%
14/03/23 15:32:48 INFO mapred.JobClient:  map 23% reduce 4%
14/03/23 15:33:03 INFO mapred.JobClient:  map 24% reduce 5%
14/03/23 15:33:09 INFO mapred.JobClient:  map 24% reduce 6%
14/03/23 15:33:15 INFO mapred.JobClient:  map 24% reduce 7%
14/03/23 15:33:21 INFO mapred.JobClient:  map 25% reduce 7%
14/03/23 15:33:30 INFO mapred.JobClient:  map 26% reduce 7%
14/03/23 15:33:42 INFO mapred.JobClient:  map 27% reduce 7%
14/03/23 15:33:51 INFO mapred.JobClient:  map 28% reduce 7%
14/03/23 15:34:03 INFO mapred.JobClient:  map 29% reduce 7%
14/03/23 15:34:12 INFO mapred.JobClient:  map 30% reduce 7%
14/03/23 15:34:24 INFO mapred.JobClient:  map 31% reduce 7%
14/03/23 15:34:33 INFO mapred.JobClient:  map 32% reduce 7%
14/03/23 15:34:45 INFO mapred.JobClient:  map 33% reduce 7%
14/03/23 15:34:57 INFO mapred.JobClient:  map 34% reduce 7%
14/03/23 15:35:06 INFO mapred.JobClient:  map 35% reduce 8%
14/03/23 15:35:21 INFO mapred.JobClient:  map 36% reduce 9%
14/03/23 15:35:27 INFO mapred.JobClient:  map 36% reduce 10%
14/03/23 15:35:33 INFO mapred.JobClient:  map 36% reduce 11%
14/03/23 15:35:36 INFO mapred.JobClient:  map 37% reduce 11%
14/03/23 15:35:48 INFO mapred.JobClient:  map 38% reduce 11%
14/03/23 15:35:57 INFO mapred.JobClient:  map 39% reduce 11%
14/03/23 15:36:09 INFO mapred.JobClient:  map 40% reduce 11%
14/03/23 15:36:18 INFO mapred.JobClient:  map 41% reduce 11%
14/03/23 15:36:27 INFO mapred.JobClient:  map 42% reduce 11%
14/03/23 15:36:39 INFO mapred.JobClient:  map 43% reduce 11%
14/03/23 15:36:48 INFO mapred.JobClient:  map 44% reduce 11%
14/03/23 15:37:00 INFO mapred.JobClient:  map 45% reduce 11%
14/03/23 15:37:09 INFO mapred.JobClient:  map 46% reduce 11%
14/03/23 15:37:21 INFO mapred.JobClient:  map 46% reduce 12%
14/03/23 15:37:24 INFO mapred.JobClient:  map 47% reduce 12%
14/03/23 15:37:36 INFO mapred.JobClient:  map 48% reduce 13%
14/03/23 15:37:42 INFO mapred.JobClient:  map 48% reduce 14%
14/03/23 15:37:48 INFO mapred.JobClient:  map 48% reduce 15%
14/03/23 15:37:51 INFO mapred.JobClient:  map 49% reduce 15%
14/03/23 15:38:03 INFO mapred.JobClient:  map 50% reduce 15%
14/03/23 15:38:12 INFO mapred.JobClient:  map 51% reduce 15%
14/03/23 15:38:24 INFO mapred.JobClient:  map 52% reduce 15%
14/03/23 15:38:33 INFO mapred.JobClient:  map 53% reduce 15%
14/03/23 15:38:45 INFO mapred.JobClient:  map 54% reduce 15%
14/03/23 15:38:54 INFO mapred.JobClient:  map 55% reduce 15%
14/03/23 15:39:03 INFO mapred.JobClient:  map 56% reduce 15%
14/03/23 15:39:15 INFO mapred.JobClient:  map 57% reduce 15%
14/03/23 15:39:24 INFO mapred.JobClient:  map 58% reduce 15%
14/03/23 15:39:36 INFO mapred.JobClient:  map 58% reduce 16%
14/03/23 15:39:39 INFO mapred.JobClient:  map 59% reduce 16%
14/03/23 15:39:42 INFO mapred.JobClient:  map 59% reduce 17%
14/03/23 15:39:51 INFO mapred.JobClient:  map 60% reduce 17%
14/03/23 15:39:58 INFO mapred.JobClient:  map 60% reduce 18%
14/03/23 15:40:07 INFO mapred.JobClient:  map 61% reduce 18%
14/03/23 15:40:13 INFO mapred.JobClient:  map 61% reduce 19%
14/03/23 15:40:16 INFO mapred.JobClient:  map 62% reduce 19%
14/03/23 15:40:28 INFO mapred.JobClient:  map 63% reduce 19%
14/03/23 15:40:40 INFO mapred.JobClient:  map 64% reduce 19%
14/03/23 15:40:49 INFO mapred.JobClient:  map 65% reduce 19%
14/03/23 15:41:01 INFO mapred.JobClient:  map 66% reduce 19%
14/03/23 15:41:10 INFO mapred.JobClient:  map 67% reduce 19%
14/03/23 15:41:22 INFO mapred.JobClient:  map 68% reduce 19%
14/03/23 15:41:31 INFO mapred.JobClient:  map 69% reduce 19%
14/03/23 15:41:43 INFO mapred.JobClient:  map 70% reduce 20%
14/03/23 15:41:55 INFO mapred.JobClient:  map 71% reduce 20%
14/03/23 15:41:58 INFO mapred.JobClient:  map 71% reduce 21%
14/03/23 15:42:07 INFO mapred.JobClient:  map 72% reduce 21%
14/03/23 15:42:13 INFO mapred.JobClient:  map 72% reduce 22%
14/03/23 15:42:22 INFO mapred.JobClient:  map 73% reduce 23%
14/03/23 15:42:34 INFO mapred.JobClient:  map 74% reduce 23%
14/03/23 15:42:43 INFO mapred.JobClient:  map 75% reduce 23%
14/03/23 15:42:55 INFO mapred.JobClient:  map 76% reduce 23%
14/03/23 15:43:04 INFO mapred.JobClient:  map 77% reduce 23%
14/03/23 15:43:13 INFO mapred.JobClient:  map 78% reduce 23%
14/03/23 15:43:22 INFO mapred.JobClient:  map 79% reduce 23%
14/03/23 15:43:34 INFO mapred.JobClient:  map 80% reduce 23%
14/03/23 15:43:43 INFO mapred.JobClient:  map 81% reduce 23%
14/03/23 15:43:55 INFO mapred.JobClient:  map 82% reduce 24%
14/03/23 15:44:07 INFO mapred.JobClient:  map 83% reduce 24%
14/03/23 15:44:16 INFO mapred.JobClient:  map 83% reduce 25%
14/03/23 15:44:19 INFO mapred.JobClient:  map 84% reduce 25%
14/03/23 15:44:31 INFO mapred.JobClient:  map 85% reduce 26%
14/03/23 15:44:37 INFO mapred.JobClient:  map 85% reduce 27%
14/03/23 15:44:43 INFO mapred.JobClient:  map 86% reduce 27%
14/03/23 15:44:55 INFO mapred.JobClient:  map 87% reduce 27%
14/03/23 15:45:04 INFO mapred.JobClient:  map 88% reduce 27%
14/03/23 15:45:13 INFO mapred.JobClient:  map 89% reduce 27%
14/03/23 15:45:22 INFO mapred.JobClient:  map 90% reduce 27%
14/03/23 15:45:31 INFO mapred.JobClient:  map 91% reduce 27%
14/03/23 15:45:43 INFO mapred.JobClient:  map 92% reduce 27%
14/03/23 15:45:52 INFO mapred.JobClient:  map 93% reduce 27%
14/03/23 15:46:01 INFO mapred.JobClient:  map 94% reduce 27%
14/03/23 15:46:07 INFO mapred.JobClient:  map 94% reduce 28%
14/03/23 15:46:13 INFO mapred.JobClient:  map 95% reduce 28%
14/03/23 15:46:22 INFO mapred.JobClient:  map 95% reduce 29%
14/03/23 15:46:25 INFO mapred.JobClient:  map 96% reduce 29%
14/03/23 15:46:37 INFO mapred.JobClient:  map 96% reduce 30%
14/03/23 15:46:40 INFO mapred.JobClient:  map 97% reduce 30%
14/03/23 15:46:46 INFO mapred.JobClient:  map 97% reduce 31%
14/03/23 15:47:04 INFO mapred.JobClient:  map 98% reduce 31%
14/03/23 15:47:34 INFO mapred.JobClient:  map 99% reduce 31%
14/03/23 15:48:04 INFO mapred.JobClient:  map 100% reduce 31%
14/03/23 15:48:07 INFO mapred.JobClient:  map 100% reduce 32%
14/03/23 15:48:16 INFO mapred.JobClient:  map 100% reduce 66%
14/03/23 15:48:19 INFO mapred.JobClient:  map 100% reduce 69%
14/03/23 15:48:22 INFO mapred.JobClient:  map 100% reduce 74%
14/03/23 15:48:25 INFO mapred.JobClient:  map 100% reduce 78%
14/03/23 15:48:28 INFO mapred.JobClient:  map 100% reduce 82%
14/03/23 15:48:31 INFO mapred.JobClient:  map 100% reduce 86%
14/03/23 15:48:34 INFO mapred.JobClient:  map 100% reduce 90%
14/03/23 15:48:37 INFO mapred.JobClient:  map 100% reduce 93%
14/03/23 15:48:40 INFO mapred.JobClient:  map 100% reduce 97%
14/03/23 15:48:46 INFO mapred.JobClient:  map 100% reduce 100%
14/03/23 15:48:51 INFO mapred.JobClient: Job complete: job_201403231525_0001
14/03/23 15:48:51 INFO mapred.JobClient: Counters: 29
14/03/23 15:48:51 INFO mapred.JobClient:   Job Counters 
14/03/23 15:48:51 INFO mapred.JobClient:     Launched reduce tasks=2
14/03/23 15:48:51 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=13331639
14/03/23 15:48:51 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
14/03/23 15:48:51 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
14/03/23 15:48:51 INFO mapred.JobClient:     Launched map tasks=100
14/03/23 15:48:51 INFO mapred.JobClient:     Data-local map tasks=100
14/03/23 15:48:51 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=2173803
14/03/23 15:48:51 INFO mapred.JobClient:   File Output Format Counters 
14/03/23 15:48:51 INFO mapred.JobClient:     Bytes Written=293004675
14/03/23 15:48:51 INFO mapred.JobClient:   FileSystemCounters
14/03/23 15:48:51 INFO mapred.JobClient:     FILE_BYTES_READ=1009300145
14/03/23 15:48:51 INFO mapred.JobClient:     HDFS_BYTES_READ=3118758693
14/03/23 15:48:51 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=2021293728
14/03/23 15:48:51 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=293004675
14/03/23 15:48:51 INFO mapred.JobClient:   File Input Format Counters 
14/03/23 15:48:51 INFO mapred.JobClient:     Bytes Read=3068323935
14/03/23 15:48:51 INFO mapred.JobClient:   Map-Reduce Framework
14/03/23 15:48:51 INFO mapred.JobClient:     Map output materialized bytes=1009301933
14/03/23 15:48:51 INFO mapred.JobClient:     Map input records=14303375
14/03/23 15:48:51 INFO mapred.JobClient:     Reduce shuffle bytes=1000218322
14/03/23 15:48:51 INFO mapred.JobClient:     Spilled Records=204800
14/03/23 15:48:51 INFO mapred.JobClient:     Map output bytes=1008902950
14/03/23 15:48:51 INFO mapred.JobClient:     CPU time spent (ms)=12966550
14/03/23 15:48:51 INFO mapred.JobClient:     Total committed heap usage (bytes)=224121978880
14/03/23 15:48:51 INFO mapred.JobClient:     Combine input records=0
14/03/23 15:48:51 INFO mapred.JobClient:     SPLIT_RAW_BYTES=13200
14/03/23 15:48:51 INFO mapred.JobClient:     Reduce input records=102400
14/03/23 15:48:51 INFO mapred.JobClient:     Reduce input groups=1024
14/03/23 15:48:51 INFO mapred.JobClient:     Combine output records=0
14/03/23 15:48:51 INFO mapred.JobClient:     Physical memory (bytes) snapshot=113019850752
14/03/23 15:48:51 INFO mapred.JobClient:     Reduce output records=1024
14/03/23 15:48:51 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=1968986910720
14/03/23 15:48:51 INFO mapred.JobClient:     Map output records=102400
14/03/23 15:48:51 INFO driver.MahoutDriver: Program took 1288075 ms (Minutes: 21.467916666666667)

real	21m32.627s
user	0m10.677s
sys	0m2.793s
jmg3     22609  0.0  0.0 106084  1396 ?        Ss   15:48   0:00 bash -c ps aux | grep java
jmg3     22625  0.0  0.0 103232   840 ?        S    15:48   0:00 grep java
jmg3     11892  0.0  0.0  59072  3528 pts/0    S+   15:48   0:00 ssh -o ConnectTimeout=2 gpu-010 ps aux | grep java
jmg3     11898  0.0  0.0 106084  1396 ?        Ss   15:48   0:00 bash -c ps aux | grep java
jmg3     11914  0.0  0.0 103232   840 ?        S    15:48   0:00 grep java
java: no process killed
java: no process killed
jmg3     22671  0.0  0.0 106084  1400 ?        Ss   15:48   0:00 bash -c ps aux | grep java
jmg3     22687  0.0  0.0 103232   840 ?        S    15:48   0:00 grep java
jmg3     11992  0.0  0.0  59072  3528 pts/0    S+   15:48   0:00 ssh -o ConnectTimeout=2 gpu-010 ps aux | grep java
jmg3     11998  0.0  0.0 106084  1400 ?        Ss   15:48   0:00 bash -c ps aux | grep java
jmg3     12014  0.0  0.0 103232   840 ?        S    15:48   0:00 grep java
Setting path to /tmp/yiskylee
12 2 0 0 0 0 67108864 16
Max. num tasks = 12 mappers, 2 reducers
Mapper thread configs = GPU ( 0 ), CPU ( 0 )
Reducer thread configs = GPU ( 0 ), CPU ( 0 )
HDFS block size = 67108864
GPU/CPU Mults shouldn't matter



Here are the results:
export JAVA_HOME=/opt/apps/jdk/1.6
export HOME=/home/jmg3
export HADOOP_HOME=${HOME}/hadoop-1.0.3
export APARAPI_HOME=${HOME}/aparapi-read-only
export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true
export HADOOPCL_KERNEL_FOLDER=${HOME}/kernels
export HADOOP_LOG_DIR=/tmp/hadoop/logs
export HADOOP_APP_DIR=${HOME}/app
export CLASSPATH=${APARAPI_HOME}/com.amd.aparapi/dist/aparapi.jar:${HADOOP_HOME}/build/hadoop-core-1.0.4-SNAPSHOT.jar:${HOME}/hadoop-gpl-compression-read-only/build/hadoop-gpl-compression-0.2.0-dev.jar:${HADOOP_HOME}/build/hadoop-core-1.0.4-SNAPSHOT.jar:${HOME}/jfreechart-1.0.14.jar:.:${CLASSPATH}:/home/jmg3/mahout/math/target/mahout-math-0.9-SNAPSHOT.jar:/home/jmg3/mahout/integration/target/dependency/mahout-core-0.9-SNAPSHOT.jar
export HADOOP_CLASSPATH=${HOME}/hadoop-gpl-compression-read-only/build/hadoop-gpl-compression-0.2.0-dev.jar:${CLASSPATH}:${HADOOP_CLASSPATH}:/home/jmg3/mahout/math/target/mahout-math-0.9-SNAPSHOT.jar:/home/jmg3/mahout/integration/target/dependency/mahout-core-0.9-SNAPSHOT.jar
export JAVA_LIBRARY_PATH=${HOME}/lzo-install/lib:${JAVA_LIBRARY_PATH}

export SCIPY_HOME=${HOME}/scipy-install
export NUMPY_HOME=${HOME}/numpy-install
export SCIKIT_HOME=${HOME}/scikit-install
export PYTHONPATH=${SCIKIT_HOME}/lib64/python2.6/site-packages:${NUMPY_HOME}/lib64/python2.6/site-packages:${SCIPY_HOME}/lib64/python2.6/site-packages:${PYTHONPATH}

-----------------------------------------------------
  <name>mapred.job.tracker</name>
  <value>gpu-010.davinci.rice.edu:54311</value>
  <name>mapred.task.tracker.http.address</name>
  <value>0.0.0.0:50061</value>
  <name>mapred.reduce.parallel.copies</name><value>5</value>
  <name>tasktracker.http.threads</name><value>40</value>
  <name>mapred.map.tasks</name><value>24</value>
  <name>mapred.reduce.tasks</name><value>2</value>
  <name>mapred.tasktracker.map.tasks.maximum</name><value>12</value>
  <name>mapred.tasktracker.reduce.tasks.maximum</name><value>2</value>
  <name>mapred.map.tasks.speculative.execution</name>
  <value>true</value>
  <name>mapred.task.timeout</name><value>3600000</value>
  <!--name>mapred.task.timeout</name><value>1200000</value-->
  <name>mapred.child.java.opts</name><value>-Xms2G -Xmx16G -Dopencl.mapper.threadsPerGroup.gpu=0 -Dopencl.mapper.threadsPerGroup.cpu=0 -Dopencl.reducer.threadsPerGroup.gpu=0 -Dopencl.reducer.threadsPerGroup.cpu=0 -Dopencl.scheduler=BALANCED -Dopencl.vectorsToBuffer=32768</value>
  <name>mapred.map.tasks.speculative.execution</name><value>false</value>
  <name>mapred.reduce.tasks.speculative.execution</name><value>false</value>
  <name>mapreduce.map.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLBalanceScheduler</value>
  <!--name>mapreduce.map.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLML4Scheduler</value-->
  <name>mapreduce.reduce.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLBalanceScheduler</value>
  <!--name>mapreduce.reduce.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLML4Scheduler</value-->
  <name>io.compression.codecs</name>
  <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec</value>
  <name>io.sort.spill.percent</name>
  <value>0.7</value>
  <name>io.sort.spill.percent.hadoopcl</name>
  <value>0.6</value>
  <name>io.sort.spill.percent.hadoopcl.initial</name>
  <value>0.5</value>
  <name>io.sort.quickrestart</name>
  <value>0.4</value>
  <name>io.sort.maxrestart</name>
  <value>0.6</value>
  <name>io.sort.mb</name>
  <value>100</value>
  <name>min.num.spills.for.combine</name>
  <value>3</value>
  <name>opencl.global.buckets</name>
  <value>65536</value>
  <name>opencl.spill.chunk</name>
  <value>2</value>
<property><name>opencl.mapper.nkernels</name><value>1</value></property>
<property><name>opencl.mapper.ninputbuffers</name><value>2</value></property>
<property><name>opencl.mapper.noutputbuffers</name><value>4</value></property>
<property><name>opencl.mapper.prealloc.length.int</name><value>1966080</value></property>
<property><name>opencl.mapper.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.mapper.prealloc.length.double</name><value>1966080</value></property>
<property><name>opencl.mapper.inputBufferSize</name><value>65536</value></property>
<property><name>opencl.mapper.outputBufferSize</name><value>65536</value></property>
<property><name>opencl.mapper.val_ele_multiplier</name><value>15</value></property>
<!--property><name>opencl.mapper.compare</name><value>true</value></property-->
<property><name>opencl.combiner.nkernels</name><value>1</value></property>
<property><name>opencl.combiner.ninputbuffers</name><value>1</value></property>
<property><name>opencl.combiner.noutputbuffers</name><value>1</value></property>
<property><name>opencl.combiner.prealloc.length.int</name><value>7340032</value></property>
<property><name>opencl.combiner.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.combiner.prealloc.length.double</name><value>6291456</value></property>
<property><name>opencl.combiner.inputBufferSize</name><value>6000</value></property>
<property><name>opencl.combiner.outputBufferSize</name><value>16384</value></property>
<property><name>opencl.combiner.val_multiplier</name><value>60</value></property>
<property><name>opencl.combiner.val_ele_multiplier</name><value>20</value></property>
<property><name>opencl.reducer.nkernels</name><value>1</value></property>
<property><name>opencl.reducer.ninputbuffers</name><value>1</value></property>
<property><name>opencl.reducer.noutputbuffers</name><value>1</value></property>
<property><name>opencl.reducer.prealloc.length.int</name><value>16777216</value></property>
<property><name>opencl.reducer.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.reducer.prealloc.length.double</name><value>16777216</value></property>
<property><name>opencl.reducer.inputBufferSize</name><value>65536</value></property>
<property><name>opencl.reducer.outputBufferSize</name><value>2048</value></property>
  <name>opencl.buffer.diagnostics</name>
  <value>false</value>
  <name>opencl.profiling</name>
  <value>false</value>
  <name>opencl.highlevel</name>
  <value>false</value>
  <name>opencl.recordings_folder</name>
  <value>/scratch/jmg3/hadoopcl-recordings/</value>
-----------------------------------------------------
  <name>dfs.safemode.threshold.pct</name>
  <value>0</value>
  <name>dfs.replication</name><value>3</value>
  <name>dfs.block.size</name><value>67108864</value>
  <name>dfs.datanode.handler.count</name><value>3</value>
  <name>dfs.namenode.handler.count</name><value>10</value>
  <name>dfs.datanode.max.xcievers</name><value>256</value>
-----------------------------------------------------
  <name>hadoop.tmp.dir</name>
  <value>/tmp/hadoop/hadoop-${user.name}</value>
  <name>fs.default.name</name>
  <value>hdfs://gpu-010.davinci.rice.edu:54313</value>
<name>io.compression.codecs</name>
<value>
</value>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
-----------------------------------------------------
gpu-009
-----------------------------------------------------
gpu-010
-----------------------------------------------------
Completed reconfiguring
Warning: $HADOOP_HOME is deprecated.

14/03/23 15:49:03 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gpu-010.davinci.rice.edu/192.168.110.210
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 1.0.4-SNAPSHOT
STARTUP_MSG:   build =  -r ; compiled by 'jmg3' on Sun Mar 23 13:28:23 CDT 2014
************************************************************/
14/03/23 15:49:03 INFO util.GSet: VM type       = 64-bit
14/03/23 15:49:03 INFO util.GSet: 2% max memory = 17.77875 MB
14/03/23 15:49:03 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/03/23 15:49:03 INFO util.GSet: recommended=2097152, actual=2097152
14/03/23 15:49:03 INFO namenode.FSNamesystem: fsOwner=jmg3
14/03/23 15:49:03 INFO namenode.FSNamesystem: supergroup=supergroup
14/03/23 15:49:03 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/03/23 15:49:03 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/03/23 15:49:03 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/03/23 15:49:03 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/03/23 15:49:03 INFO common.Storage: Image file of size 110 saved in 0 seconds.
14/03/23 15:49:03 INFO common.Storage: Storage directory /tmp/hadoop/hadoop-jmg3/dfs/name has been successfully formatted.
14/03/23 15:49:03 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gpu-010.davinci.rice.edu/192.168.110.210
************************************************************/
Completed namenode startup
Warning: $HADOOP_HOME is deprecated.

no jobtracker to stop
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: no tasktracker to stop
no namenode to stop
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: no datanode to stop
gpu-010: Warning: $HADOOP_HOME is deprecated.
gpu-010: 
gpu-010: no secondarynamenode to stop
Completed stop all
Warning: $HADOOP_HOME is deprecated.

starting namenode, logging to /tmp/hadoop/logs/hadoop-jmg3-namenode-gpu-010.davinci.rice.edu.out
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: starting datanode, logging to /tmp/hadoop/logs/hadoop-jmg3-datanode-gpu-009.davinci.rice.edu.out
gpu-010: Warning: $HADOOP_HOME is deprecated.
gpu-010: 
gpu-010: starting secondarynamenode, logging to /tmp/hadoop/logs/hadoop-jmg3-secondarynamenode-gpu-010.davinci.rice.edu.out
starting jobtracker, logging to /tmp/hadoop/logs/hadoop-jmg3-jobtracker-gpu-010.davinci.rice.edu.out
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: starting tasktracker, logging to /tmp/hadoop/logs/hadoop-jmg3-tasktracker-gpu-009.davinci.rice.edu.out
Completed start all
Warning: $HADOOP_HOME is deprecated.

Warning: $HADOOP_HOME is deprecated.

Warning: $HADOOP_HOME is deprecated.

MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.
Warning: $HADOOP_HOME is deprecated.

Running on hadoop, using /home/jmg3/hadoop-1.0.3/bin/hadoop and HADOOP_CONF_DIR=/home/jmg3/hadoop-1.0.3/conf
MAHOUT-JOB: /home/jmg3/mahout/examples/target/mahout-examples-0.9-SNAPSHOT-job.jar
Warning: $HADOOP_HOME is deprecated.

14/03/23 15:51:27 INFO common.AbstractJob: Command line arguments: {--clusters=[clusters], --convergenceDelta=[0.5], --distanceMeasure=[org.apache.mahout.common.distance.CosineDistanceMeasure], --endPhase=[2147483647], --input=[input], --maxIter=[1], --method=[mapreduce], --output=[output], --overwrite=null, --startPhase=[0], --tempDir=[temp]}
14/03/23 15:51:28 INFO kmeans.KMeansDriver: Input: input Clusters In: clusters Out: output Distance: org.apache.mahout.common.distance.CosineDistanceMeasure
14/03/23 15:51:28 INFO kmeans.KMeansDriver: convergence: 0.5 max Iterations: 1
14/03/23 15:51:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/03/23 15:51:28 INFO compress.CodecPool: Got brand-new decompressor
Cluster Iterator running iteration 1/1 over priorPath: output/clusters-0
Sending 0 to HDFS took 198 ms
14/03/23 15:52:03 INFO input.FileInputFormat: Total input paths to process : 100
14/03/23 15:52:05 INFO mapred.JobClient: Running job: job_201403231549_0001
14/03/23 15:52:06 INFO mapred.JobClient:  map 0% reduce 0%
14/03/23 15:52:34 INFO mapred.JobClient:  map 1% reduce 0%
14/03/23 15:52:46 INFO mapred.JobClient:  map 2% reduce 0%
14/03/23 15:52:58 INFO mapred.JobClient:  map 3% reduce 0%
14/03/23 15:53:10 INFO mapred.JobClient:  map 4% reduce 0%
14/03/23 15:53:22 INFO mapred.JobClient:  map 5% reduce 0%
14/03/23 15:53:34 INFO mapred.JobClient:  map 6% reduce 0%
14/03/23 15:53:43 INFO mapred.JobClient:  map 7% reduce 0%
14/03/23 15:53:55 INFO mapred.JobClient:  map 8% reduce 0%
14/03/23 15:54:07 INFO mapred.JobClient:  map 9% reduce 0%
14/03/23 15:54:16 INFO mapred.JobClient:  map 10% reduce 0%
14/03/23 15:54:28 INFO mapred.JobClient:  map 11% reduce 0%
14/03/23 15:54:46 INFO mapred.JobClient:  map 12% reduce 1%
14/03/23 15:54:49 INFO mapred.JobClient:  map 12% reduce 3%
14/03/23 15:55:01 INFO mapred.JobClient:  map 13% reduce 3%
14/03/23 15:55:13 INFO mapred.JobClient:  map 14% reduce 3%
14/03/23 15:55:22 INFO mapred.JobClient:  map 15% reduce 3%
14/03/23 15:55:34 INFO mapred.JobClient:  map 16% reduce 3%
14/03/23 15:55:46 INFO mapred.JobClient:  map 17% reduce 3%
14/03/23 15:55:55 INFO mapred.JobClient:  map 18% reduce 3%
14/03/23 15:56:07 INFO mapred.JobClient:  map 19% reduce 3%
14/03/23 15:56:19 INFO mapred.JobClient:  map 20% reduce 3%
14/03/23 15:56:28 INFO mapred.JobClient:  map 21% reduce 3%
14/03/23 15:56:40 INFO mapred.JobClient:  map 22% reduce 3%
14/03/23 15:56:49 INFO mapred.JobClient:  map 23% reduce 3%
14/03/23 15:56:58 INFO mapred.JobClient:  map 23% reduce 4%
14/03/23 15:57:01 INFO mapred.JobClient:  map 23% reduce 5%
14/03/23 15:57:04 INFO mapred.JobClient:  map 23% reduce 6%
14/03/23 15:57:07 INFO mapred.JobClient:  map 24% reduce 6%
14/03/23 15:57:13 INFO mapred.JobClient:  map 24% reduce 7%
14/03/23 15:57:19 INFO mapred.JobClient:  map 25% reduce 7%
14/03/23 15:57:31 INFO mapred.JobClient:  map 26% reduce 7%
14/03/23 15:57:43 INFO mapred.JobClient:  map 27% reduce 7%
14/03/23 15:57:53 INFO mapred.JobClient:  map 28% reduce 7%
14/03/23 15:58:05 INFO mapred.JobClient:  map 29% reduce 7%
14/03/23 15:58:14 INFO mapred.JobClient:  map 30% reduce 7%
14/03/23 15:58:26 INFO mapred.JobClient:  map 31% reduce 7%
14/03/23 15:58:36 INFO mapred.JobClient:  map 32% reduce 7%
14/03/23 15:58:45 INFO mapred.JobClient:  map 33% reduce 7%
14/03/23 15:58:57 INFO mapred.JobClient:  map 34% reduce 7%
14/03/23 15:59:09 INFO mapred.JobClient:  map 35% reduce 7%
14/03/23 15:59:15 INFO mapred.JobClient:  map 35% reduce 8%
14/03/23 15:59:27 INFO mapred.JobClient:  map 36% reduce 10%
14/03/23 15:59:33 INFO mapred.JobClient:  map 36% reduce 11%
14/03/23 15:59:39 INFO mapred.JobClient:  map 37% reduce 11%
14/03/23 15:59:51 INFO mapred.JobClient:  map 38% reduce 11%
14/03/23 16:00:00 INFO mapred.JobClient:  map 39% reduce 11%
14/03/23 16:00:12 INFO mapred.JobClient:  map 40% reduce 11%
14/03/23 16:00:20 INFO mapred.JobClient:  map 41% reduce 11%
14/03/23 16:00:32 INFO mapred.JobClient:  map 42% reduce 11%
14/03/23 16:00:41 INFO mapred.JobClient:  map 43% reduce 11%
14/03/23 16:00:50 INFO mapred.JobClient:  map 44% reduce 11%
14/03/23 16:01:02 INFO mapred.JobClient:  map 45% reduce 11%
14/03/23 16:01:14 INFO mapred.JobClient:  map 46% reduce 11%
14/03/23 16:01:26 INFO mapred.JobClient:  map 47% reduce 11%
14/03/23 16:01:29 INFO mapred.JobClient:  map 47% reduce 12%
14/03/23 16:01:38 INFO mapred.JobClient:  map 47% reduce 13%
14/03/23 16:01:41 INFO mapred.JobClient:  map 48% reduce 14%
14/03/23 16:01:50 INFO mapred.JobClient:  map 48% reduce 15%
14/03/23 16:01:53 INFO mapred.JobClient:  map 49% reduce 15%
14/03/23 16:02:05 INFO mapred.JobClient:  map 50% reduce 15%
14/03/23 16:02:17 INFO mapred.JobClient:  map 51% reduce 15%
14/03/23 16:02:26 INFO mapred.JobClient:  map 52% reduce 15%
14/03/23 16:02:38 INFO mapred.JobClient:  map 53% reduce 15%
14/03/23 16:02:47 INFO mapred.JobClient:  map 54% reduce 15%
14/03/23 16:02:59 INFO mapred.JobClient:  map 55% reduce 15%
14/03/23 16:03:08 INFO mapred.JobClient:  map 56% reduce 15%
14/03/23 16:03:20 INFO mapred.JobClient:  map 57% reduce 15%
14/03/23 16:03:29 INFO mapred.JobClient:  map 58% reduce 15%
14/03/23 16:03:35 INFO mapred.JobClient:  map 58% reduce 16%
14/03/23 16:03:41 INFO mapred.JobClient:  map 59% reduce 16%
14/03/23 16:03:53 INFO mapred.JobClient:  map 59% reduce 17%
14/03/23 16:03:56 INFO mapred.JobClient:  map 60% reduce 18%
14/03/23 16:04:08 INFO mapred.JobClient:  map 61% reduce 18%
14/03/23 16:04:20 INFO mapred.JobClient:  map 62% reduce 19%
14/03/23 16:04:32 INFO mapred.JobClient:  map 63% reduce 19%
14/03/23 16:04:41 INFO mapred.JobClient:  map 64% reduce 19%
14/03/23 16:04:53 INFO mapred.JobClient:  map 65% reduce 19%
14/03/23 16:05:02 INFO mapred.JobClient:  map 66% reduce 19%
14/03/23 16:05:11 INFO mapred.JobClient:  map 67% reduce 19%
14/03/23 16:05:23 INFO mapred.JobClient:  map 68% reduce 19%
14/03/23 16:05:35 INFO mapred.JobClient:  map 69% reduce 19%
14/03/23 16:05:44 INFO mapred.JobClient:  map 70% reduce 19%
14/03/23 16:05:47 INFO mapred.JobClient:  map 70% reduce 20%
14/03/23 16:05:56 INFO mapred.JobClient:  map 71% reduce 20%
14/03/23 16:06:02 INFO mapred.JobClient:  map 71% reduce 21%
14/03/23 16:06:05 INFO mapred.JobClient:  map 71% reduce 22%
14/03/23 16:06:08 INFO mapred.JobClient:  map 72% reduce 22%
14/03/23 16:06:20 INFO mapred.JobClient:  map 73% reduce 22%
14/03/23 16:06:23 INFO mapred.JobClient:  map 73% reduce 23%
14/03/23 16:06:32 INFO mapred.JobClient:  map 74% reduce 23%
14/03/23 16:06:44 INFO mapred.JobClient:  map 75% reduce 23%
14/03/23 16:06:53 INFO mapred.JobClient:  map 76% reduce 23%
14/03/23 16:07:02 INFO mapred.JobClient:  map 77% reduce 23%
14/03/23 16:07:14 INFO mapred.JobClient:  map 78% reduce 23%
14/03/23 16:07:23 INFO mapred.JobClient:  map 79% reduce 23%
14/03/23 16:07:32 INFO mapred.JobClient:  map 80% reduce 23%
14/03/23 16:07:44 INFO mapred.JobClient:  map 81% reduce 23%
14/03/23 16:07:53 INFO mapred.JobClient:  map 82% reduce 24%
14/03/23 16:08:05 INFO mapred.JobClient:  map 83% reduce 24%
14/03/23 16:08:14 INFO mapred.JobClient:  map 83% reduce 25%
14/03/23 16:08:20 INFO mapred.JobClient:  map 84% reduce 25%
14/03/23 16:08:26 INFO mapred.JobClient:  map 84% reduce 26%
14/03/23 16:08:32 INFO mapred.JobClient:  map 85% reduce 26%
14/03/23 16:08:35 INFO mapred.JobClient:  map 85% reduce 27%
14/03/23 16:08:45 INFO mapred.JobClient:  map 86% reduce 27%
14/03/23 16:08:54 INFO mapred.JobClient:  map 87% reduce 27%
14/03/23 16:09:06 INFO mapred.JobClient:  map 88% reduce 27%
14/03/23 16:09:15 INFO mapred.JobClient:  map 89% reduce 27%
14/03/23 16:09:24 INFO mapred.JobClient:  map 90% reduce 27%
14/03/23 16:09:33 INFO mapred.JobClient:  map 91% reduce 27%
14/03/23 16:09:42 INFO mapred.JobClient:  map 92% reduce 27%
14/03/23 16:09:54 INFO mapred.JobClient:  map 93% reduce 27%
14/03/23 16:10:06 INFO mapred.JobClient:  map 94% reduce 28%
14/03/23 16:10:15 INFO mapred.JobClient:  map 95% reduce 28%
14/03/23 16:10:21 INFO mapred.JobClient:  map 95% reduce 29%
14/03/23 16:10:30 INFO mapred.JobClient:  map 96% reduce 30%
14/03/23 16:10:45 INFO mapred.JobClient:  map 97% reduce 30%
14/03/23 16:10:51 INFO mapred.JobClient:  map 97% reduce 31%
14/03/23 16:11:12 INFO mapred.JobClient:  map 98% reduce 31%
14/03/23 16:11:39 INFO mapred.JobClient:  map 99% reduce 31%
14/03/23 16:12:06 INFO mapred.JobClient:  map 99% reduce 32%
14/03/23 16:12:18 INFO mapred.JobClient:  map 100% reduce 32%
14/03/23 16:12:27 INFO mapred.JobClient:  map 100% reduce 33%
14/03/23 16:12:30 INFO mapred.JobClient:  map 100% reduce 66%
14/03/23 16:12:36 INFO mapred.JobClient:  map 100% reduce 68%
14/03/23 16:12:39 INFO mapred.JobClient:  map 100% reduce 74%
14/03/23 16:12:42 INFO mapred.JobClient:  map 100% reduce 79%
14/03/23 16:12:45 INFO mapred.JobClient:  map 100% reduce 83%
14/03/23 16:12:48 INFO mapred.JobClient:  map 100% reduce 86%
14/03/23 16:12:51 INFO mapred.JobClient:  map 100% reduce 91%
14/03/23 16:12:54 INFO mapred.JobClient:  map 100% reduce 93%
14/03/23 16:12:57 INFO mapred.JobClient:  map 100% reduce 97%
14/03/23 16:13:03 INFO mapred.JobClient:  map 100% reduce 100%
14/03/23 16:13:08 INFO mapred.JobClient: Job complete: job_201403231549_0001
14/03/23 16:13:08 INFO mapred.JobClient: Counters: 29
14/03/23 16:13:08 INFO mapred.JobClient:   Job Counters 
14/03/23 16:13:08 INFO mapred.JobClient:     Launched reduce tasks=2
14/03/23 16:13:08 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=13365489
14/03/23 16:13:08 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
14/03/23 16:13:08 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
14/03/23 16:13:08 INFO mapred.JobClient:     Launched map tasks=100
14/03/23 16:13:08 INFO mapred.JobClient:     Data-local map tasks=100
14/03/23 16:13:08 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=2204425
14/03/23 16:13:08 INFO mapred.JobClient:   File Output Format Counters 
14/03/23 16:13:08 INFO mapred.JobClient:     Bytes Written=293004687
14/03/23 16:13:08 INFO mapred.JobClient:   FileSystemCounters
14/03/23 16:13:08 INFO mapred.JobClient:     FILE_BYTES_READ=1009300145
14/03/23 16:13:08 INFO mapred.JobClient:     HDFS_BYTES_READ=3118758693
14/03/23 16:13:08 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=2021293728
14/03/23 16:13:08 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=293004687
14/03/23 16:13:08 INFO mapred.JobClient:   File Input Format Counters 
14/03/23 16:13:08 INFO mapred.JobClient:     Bytes Read=3068323935
14/03/23 16:13:08 INFO mapred.JobClient:   Map-Reduce Framework
14/03/23 16:13:08 INFO mapred.JobClient:     Map output materialized bytes=1009301933
14/03/23 16:13:08 INFO mapred.JobClient:     Map input records=14303375
14/03/23 16:13:08 INFO mapred.JobClient:     Reduce shuffle bytes=1000218322
14/03/23 16:13:08 INFO mapred.JobClient:     Spilled Records=204800
14/03/23 16:13:08 INFO mapred.JobClient:     Map output bytes=1008902950
14/03/23 16:13:08 INFO mapred.JobClient:     CPU time spent (ms)=13004710
14/03/23 16:13:08 INFO mapred.JobClient:     Total committed heap usage (bytes)=226183151616
14/03/23 16:13:08 INFO mapred.JobClient:     Combine input records=0
14/03/23 16:13:08 INFO mapred.JobClient:     SPLIT_RAW_BYTES=13200
14/03/23 16:13:08 INFO mapred.JobClient:     Reduce input records=102400
14/03/23 16:13:08 INFO mapred.JobClient:     Reduce input groups=1024
14/03/23 16:13:08 INFO mapred.JobClient:     Combine output records=0
14/03/23 16:13:08 INFO mapred.JobClient:     Physical memory (bytes) snapshot=113176453120
14/03/23 16:13:08 INFO mapred.JobClient:     Reduce output records=1024
14/03/23 16:13:08 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=1968983752704
14/03/23 16:13:08 INFO mapred.JobClient:     Map output records=102400
14/03/23 16:13:08 INFO driver.MahoutDriver: Program took 1300771 ms (Minutes: 21.679516666666668)

real	21m45.179s
user	0m10.561s
sys	0m2.786s

java: no process killed
java: no process killed
jmg3     16989  0.0  0.0 106084  1400 ?        Ss   13:29   0:00 bash -c ps aux | grep java
jmg3     17005  0.0  0.0 103232   844 ?        S    13:29   0:00 grep java
jmg3     26198  0.0  0.0  59204  3532 pts/0    S+   13:29   0:00 ssh -o ConnectTimeout=2 gpu-010 ps aux | grep java
jmg3     26204  0.0  0.0 106084  1400 ?        Ss   13:29   0:00 bash -c ps aux | grep java
jmg3     26220  0.0  0.0 103232   844 ?        S    13:29   0:00 grep java
java: no process killed
java: no process killed
jmg3     17051  0.0  0.0 106084  1396 ?        Ss   13:29   0:00 bash -c ps aux | grep java
jmg3     17067  0.0  0.0 103232   844 ?        S    13:29   0:00 grep java
jmg3     26282  0.0  0.0  59072  3532 pts/0    S+   13:29   0:00 ssh -o ConnectTimeout=2 gpu-010 ps aux | grep java
jmg3     26288  0.0  0.0 106084  1400 ?        Ss   13:29   0:00 bash -c ps aux | grep java
jmg3     26304  0.0  0.0 103232   844 ?        S    13:29   0:00 grep java
Setting path to /tmp/yiskylee
12 2 0 0 0 0 67108864 16
Max. num tasks = 12 mappers, 2 reducers
Mapper thread configs = GPU ( 0 ), CPU ( 0 )
Reducer thread configs = GPU ( 0 ), CPU ( 0 )
HDFS block size = 67108864
GPU/CPU Mults shouldn't matter



Here are the results:
export JAVA_HOME=/opt/apps/jdk/1.6
export HOME=/home/jmg3
export HADOOP_HOME=${HOME}/hadoop-1.0.3
export APARAPI_HOME=${HOME}/aparapi-read-only
export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true
export HADOOPCL_KERNEL_FOLDER=${HOME}/kernels
export HADOOP_LOG_DIR=/tmp/hadoop/logs
export HADOOP_APP_DIR=${HOME}/app
export CLASSPATH=${APARAPI_HOME}/com.amd.aparapi/dist/aparapi.jar:${HADOOP_HOME}/build/hadoop-core-1.0.4-SNAPSHOT.jar:${HOME}/hadoop-gpl-compression-read-only/build/hadoop-gpl-compression-0.2.0-dev.jar:${HADOOP_HOME}/build/hadoop-core-1.0.4-SNAPSHOT.jar:${HOME}/jfreechart-1.0.14.jar:.:${CLASSPATH}:/home/jmg3/mahout/math/target/mahout-math-0.9-SNAPSHOT.jar:/home/jmg3/mahout/integration/target/dependency/mahout-core-0.9-SNAPSHOT.jar
export HADOOP_CLASSPATH=${HOME}/hadoop-gpl-compression-read-only/build/hadoop-gpl-compression-0.2.0-dev.jar:${CLASSPATH}:${HADOOP_CLASSPATH}:/home/jmg3/mahout/math/target/mahout-math-0.9-SNAPSHOT.jar:/home/jmg3/mahout/integration/target/dependency/mahout-core-0.9-SNAPSHOT.jar
export JAVA_LIBRARY_PATH=${HOME}/lzo-install/lib:${JAVA_LIBRARY_PATH}

export SCIPY_HOME=${HOME}/scipy-install
export NUMPY_HOME=${HOME}/numpy-install
export SCIKIT_HOME=${HOME}/scikit-install
export PYTHONPATH=${SCIKIT_HOME}/lib64/python2.6/site-packages:${NUMPY_HOME}/lib64/python2.6/site-packages:${SCIPY_HOME}/lib64/python2.6/site-packages:${PYTHONPATH}

-----------------------------------------------------
  <name>mapred.job.tracker</name>
  <value>gpu-010.davinci.rice.edu:54311</value>
  <name>mapred.task.tracker.http.address</name>
  <value>0.0.0.0:50061</value>
  <name>mapred.reduce.parallel.copies</name><value>5</value>
  <name>tasktracker.http.threads</name><value>40</value>
  <name>mapred.map.tasks</name><value>24</value>
  <name>mapred.reduce.tasks</name><value>2</value>
  <name>mapred.tasktracker.map.tasks.maximum</name><value>12</value>
  <name>mapred.tasktracker.reduce.tasks.maximum</name><value>2</value>
  <name>mapred.map.tasks.speculative.execution</name>
  <value>true</value>
  <name>mapred.task.timeout</name><value>3600000</value>
  <!--name>mapred.task.timeout</name><value>1200000</value-->
  <name>mapred.child.java.opts</name><value>-Xms2G -Xmx16G -Dopencl.mapper.threadsPerGroup.gpu=0 -Dopencl.mapper.threadsPerGroup.cpu=0 -Dopencl.reducer.threadsPerGroup.gpu=0 -Dopencl.reducer.threadsPerGroup.cpu=0 -Dopencl.scheduler=BALANCED -Dopencl.vectorsToBuffer=32768</value>
  <name>mapred.map.tasks.speculative.execution</name><value>false</value>
  <name>mapred.reduce.tasks.speculative.execution</name><value>false</value>
  <name>mapreduce.map.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLBalanceScheduler</value>
  <!--name>mapreduce.map.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLML4Scheduler</value-->
  <name>mapreduce.reduce.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLBalanceScheduler</value>
  <!--name>mapreduce.reduce.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLML4Scheduler</value-->
  <name>io.compression.codecs</name>
  <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec</value>
  <name>io.sort.spill.percent</name>
  <value>0.7</value>
  <name>io.sort.spill.percent.hadoopcl</name>
  <value>0.6</value>
  <name>io.sort.spill.percent.hadoopcl.initial</name>
  <value>0.5</value>
  <name>io.sort.quickrestart</name>
  <value>0.4</value>
  <name>io.sort.maxrestart</name>
  <value>0.6</value>
  <name>io.sort.mb</name>
  <value>100</value>
  <name>min.num.spills.for.combine</name>
  <value>3</value>
  <name>opencl.global.buckets</name>
  <value>65536</value>
  <name>opencl.spill.chunk</name>
  <value>2</value>
<property><name>opencl.mapper.nkernels</name><value>1</value></property>
<property><name>opencl.mapper.ninputbuffers</name><value>2</value></property>
<property><name>opencl.mapper.noutputbuffers</name><value>4</value></property>
<property><name>opencl.mapper.prealloc.length.int</name><value>1966080</value></property>
<property><name>opencl.mapper.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.mapper.prealloc.length.double</name><value>1966080</value></property>
<property><name>opencl.mapper.inputBufferSize</name><value>65536</value></property>
<property><name>opencl.mapper.outputBufferSize</name><value>65536</value></property>
<property><name>opencl.mapper.val_ele_multiplier</name><value>15</value></property>
<!--property><name>opencl.mapper.compare</name><value>true</value></property-->
<property><name>opencl.combiner.nkernels</name><value>1</value></property>
<property><name>opencl.combiner.ninputbuffers</name><value>1</value></property>
<property><name>opencl.combiner.noutputbuffers</name><value>1</value></property>
<property><name>opencl.combiner.prealloc.length.int</name><value>7340032</value></property>
<property><name>opencl.combiner.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.combiner.prealloc.length.double</name><value>6291456</value></property>
<property><name>opencl.combiner.inputBufferSize</name><value>6000</value></property>
<property><name>opencl.combiner.outputBufferSize</name><value>16384</value></property>
<property><name>opencl.combiner.val_multiplier</name><value>60</value></property>
<property><name>opencl.combiner.val_ele_multiplier</name><value>20</value></property>
<property><name>opencl.reducer.nkernels</name><value>1</value></property>
<property><name>opencl.reducer.ninputbuffers</name><value>1</value></property>
<property><name>opencl.reducer.noutputbuffers</name><value>1</value></property>
<property><name>opencl.reducer.prealloc.length.int</name><value>16777216</value></property>
<property><name>opencl.reducer.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.reducer.prealloc.length.double</name><value>16777216</value></property>
<property><name>opencl.reducer.inputBufferSize</name><value>65536</value></property>
<property><name>opencl.reducer.outputBufferSize</name><value>2048</value></property>
  <name>opencl.buffer.diagnostics</name>
  <value>false</value>
  <name>opencl.profiling</name>
  <value>false</value>
  <name>opencl.highlevel</name>
  <value>false</value>
  <name>opencl.recordings_folder</name>
  <value>/scratch/jmg3/hadoopcl-recordings/</value>
-----------------------------------------------------
  <name>dfs.safemode.threshold.pct</name>
  <value>0</value>
  <name>dfs.replication</name><value>3</value>
  <name>dfs.block.size</name><value>67108864</value>
  <name>dfs.datanode.handler.count</name><value>3</value>
  <name>dfs.namenode.handler.count</name><value>10</value>
  <name>dfs.datanode.max.xcievers</name><value>256</value>
-----------------------------------------------------
  <name>hadoop.tmp.dir</name>
  <value>/tmp/hadoop/hadoop-${user.name}</value>
  <name>fs.default.name</name>
  <value>hdfs://gpu-010.davinci.rice.edu:54313</value>
<name>io.compression.codecs</name>
<value>
</value>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
-----------------------------------------------------
gpu-009
-----------------------------------------------------
gpu-010
-----------------------------------------------------
Completed reconfiguring
Warning: $HADOOP_HOME is deprecated.

14/03/23 13:29:42 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gpu-010.davinci.rice.edu/192.168.110.210
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 1.0.4-SNAPSHOT
STARTUP_MSG:   build =  -r ; compiled by 'jmg3' on Sun Mar 23 13:28:23 CDT 2014
************************************************************/
14/03/23 13:29:42 INFO util.GSet: VM type       = 64-bit
14/03/23 13:29:42 INFO util.GSet: 2% max memory = 17.77875 MB
14/03/23 13:29:42 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/03/23 13:29:42 INFO util.GSet: recommended=2097152, actual=2097152
14/03/23 13:29:43 INFO namenode.FSNamesystem: fsOwner=jmg3
14/03/23 13:29:43 INFO namenode.FSNamesystem: supergroup=supergroup
14/03/23 13:29:43 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/03/23 13:29:43 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/03/23 13:29:43 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/03/23 13:29:43 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/03/23 13:29:43 INFO common.Storage: Image file of size 110 saved in 0 seconds.
14/03/23 13:29:43 INFO common.Storage: Storage directory /tmp/hadoop/hadoop-jmg3/dfs/name has been successfully formatted.
14/03/23 13:29:43 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gpu-010.davinci.rice.edu/192.168.110.210
************************************************************/
Completed namenode startup
Warning: $HADOOP_HOME is deprecated.

no jobtracker to stop
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: no tasktracker to stop
no namenode to stop
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: no datanode to stop
gpu-010: Warning: $HADOOP_HOME is deprecated.
gpu-010: 
gpu-010: no secondarynamenode to stop
Completed stop all
Warning: $HADOOP_HOME is deprecated.

starting namenode, logging to /tmp/hadoop/logs/hadoop-jmg3-namenode-gpu-010.davinci.rice.edu.out
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: starting datanode, logging to /tmp/hadoop/logs/hadoop-jmg3-datanode-gpu-009.davinci.rice.edu.out
gpu-010: Warning: $HADOOP_HOME is deprecated.
gpu-010: 
gpu-010: starting secondarynamenode, logging to /tmp/hadoop/logs/hadoop-jmg3-secondarynamenode-gpu-010.davinci.rice.edu.out
starting jobtracker, logging to /tmp/hadoop/logs/hadoop-jmg3-jobtracker-gpu-010.davinci.rice.edu.out
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: starting tasktracker, logging to /tmp/hadoop/logs/hadoop-jmg3-tasktracker-gpu-009.davinci.rice.edu.out
Completed start all
Warning: $HADOOP_HOME is deprecated.

Warning: $HADOOP_HOME is deprecated.

Warning: $HADOOP_HOME is deprecated.

MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.
Warning: $HADOOP_HOME is deprecated.

Running on hadoop, using /home/jmg3/hadoop-1.0.3/bin/hadoop and HADOOP_CONF_DIR=/home/jmg3/hadoop-1.0.3/conf
MAHOUT-JOB: /home/jmg3/mahout/examples/target/mahout-examples-0.9-SNAPSHOT-job.jar
Warning: $HADOOP_HOME is deprecated.

14/03/23 13:32:04 INFO common.AbstractJob: Command line arguments: {--clusters=[clusters], --convergenceDelta=[0.5], --distanceMeasure=[org.apache.mahout.common.distance.CosineDistanceMeasure], --endPhase=[2147483647], --input=[input], --maxIter=[1], --method=[mapreduce], --output=[output], --overwrite=null, --startPhase=[0], --tempDir=[temp]}
14/03/23 13:32:04 INFO kmeans.KMeansDriver: Input: input Clusters In: clusters Out: output Distance: org.apache.mahout.common.distance.CosineDistanceMeasure
14/03/23 13:32:04 INFO kmeans.KMeansDriver: convergence: 0.5 max Iterations: 1
14/03/23 13:32:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/03/23 13:32:04 INFO compress.CodecPool: Got brand-new decompressor
Cluster Iterator running iteration 1/1 over priorPath: output/clusters-0
Sending 0 to HDFS took 1423 ms
14/03/23 13:32:43 INFO input.FileInputFormat: Total input paths to process : 100
14/03/23 13:32:44 INFO mapred.JobClient: Running job: job_201403231329_0001
14/03/23 13:32:45 INFO mapred.JobClient:  map 0% reduce 0%
14/03/23 13:33:16 INFO mapred.JobClient:  map 1% reduce 0%
14/03/23 13:33:28 INFO mapred.JobClient:  map 2% reduce 0%
14/03/23 13:33:40 INFO mapred.JobClient:  map 3% reduce 0%
14/03/23 13:33:49 INFO mapred.JobClient:  map 4% reduce 0%
14/03/23 13:34:04 INFO mapred.JobClient:  map 5% reduce 0%
14/03/23 13:34:16 INFO mapred.JobClient:  map 6% reduce 0%
14/03/23 13:34:25 INFO mapred.JobClient:  map 7% reduce 0%
14/03/23 13:34:37 INFO mapred.JobClient:  map 8% reduce 0%
14/03/23 13:34:46 INFO mapred.JobClient:  map 9% reduce 0%
14/03/23 13:34:58 INFO mapred.JobClient:  map 10% reduce 0%
14/03/23 13:35:10 INFO mapred.JobClient:  map 11% reduce 0%
14/03/23 13:35:28 INFO mapred.JobClient:  map 12% reduce 0%
14/03/23 13:35:31 INFO mapred.JobClient:  map 12% reduce 3%
14/03/23 13:35:43 INFO mapred.JobClient:  map 13% reduce 3%
14/03/23 13:35:55 INFO mapred.JobClient:  map 14% reduce 3%
14/03/23 13:36:04 INFO mapred.JobClient:  map 15% reduce 3%
14/03/23 13:36:16 INFO mapred.JobClient:  map 16% reduce 3%
14/03/23 13:36:25 INFO mapred.JobClient:  map 17% reduce 3%
14/03/23 13:36:37 INFO mapred.JobClient:  map 18% reduce 3%
14/03/23 13:36:46 INFO mapred.JobClient:  map 19% reduce 3%
14/03/23 13:36:58 INFO mapred.JobClient:  map 20% reduce 3%
14/03/23 13:37:09 INFO mapred.JobClient:  map 21% reduce 3%
14/03/23 13:37:18 INFO mapred.JobClient:  map 22% reduce 3%
14/03/23 13:37:30 INFO mapred.JobClient:  map 23% reduce 3%
14/03/23 13:37:39 INFO mapred.JobClient:  map 23% reduce 4%
14/03/23 13:37:42 INFO mapred.JobClient:  map 23% reduce 5%
14/03/23 13:37:45 INFO mapred.JobClient:  map 24% reduce 5%
14/03/23 13:37:51 INFO mapred.JobClient:  map 24% reduce 7%
14/03/23 13:38:00 INFO mapred.JobClient:  map 25% reduce 7%
14/03/23 13:38:12 INFO mapred.JobClient:  map 26% reduce 7%
14/03/23 13:38:21 INFO mapred.JobClient:  map 27% reduce 7%
14/03/23 13:38:33 INFO mapred.JobClient:  map 28% reduce 7%
14/03/23 13:38:42 INFO mapred.JobClient:  map 29% reduce 7%
14/03/23 13:38:54 INFO mapred.JobClient:  map 30% reduce 7%
14/03/23 13:39:03 INFO mapred.JobClient:  map 31% reduce 7%
14/03/23 13:39:12 INFO mapred.JobClient:  map 32% reduce 7%
14/03/23 13:39:25 INFO mapred.JobClient:  map 33% reduce 7%
14/03/23 13:39:37 INFO mapred.JobClient:  map 34% reduce 7%
14/03/23 13:39:46 INFO mapred.JobClient:  map 35% reduce 8%
14/03/23 13:40:01 INFO mapred.JobClient:  map 36% reduce 9%
14/03/23 13:40:10 INFO mapred.JobClient:  map 36% reduce 11%
14/03/23 13:40:16 INFO mapred.JobClient:  map 37% reduce 11%
14/03/23 13:40:28 INFO mapred.JobClient:  map 38% reduce 11%
14/03/23 13:40:37 INFO mapred.JobClient:  map 39% reduce 11%
14/03/23 13:40:49 INFO mapred.JobClient:  map 40% reduce 11%
14/03/23 13:40:58 INFO mapred.JobClient:  map 41% reduce 11%
14/03/23 13:41:07 INFO mapred.JobClient:  map 42% reduce 11%
14/03/23 13:41:19 INFO mapred.JobClient:  map 43% reduce 11%
14/03/23 13:41:28 INFO mapred.JobClient:  map 44% reduce 11%
14/03/23 13:41:40 INFO mapred.JobClient:  map 45% reduce 11%
14/03/23 13:41:49 INFO mapred.JobClient:  map 46% reduce 11%
14/03/23 13:42:01 INFO mapred.JobClient:  map 47% reduce 12%
14/03/23 13:42:13 INFO mapred.JobClient:  map 48% reduce 12%
14/03/23 13:42:16 INFO mapred.JobClient:  map 48% reduce 13%
14/03/23 13:42:25 INFO mapred.JobClient:  map 48% reduce 15%
14/03/23 13:42:31 INFO mapred.JobClient:  map 49% reduce 15%
14/03/23 13:42:43 INFO mapred.JobClient:  map 50% reduce 15%
14/03/23 13:42:52 INFO mapred.JobClient:  map 51% reduce 15%
14/03/23 13:43:01 INFO mapred.JobClient:  map 52% reduce 15%
14/03/23 13:43:10 INFO mapred.JobClient:  map 53% reduce 15%
14/03/23 13:43:25 INFO mapred.JobClient:  map 54% reduce 15%
14/03/23 13:43:34 INFO mapred.JobClient:  map 55% reduce 15%
14/03/23 13:43:43 INFO mapred.JobClient:  map 56% reduce 15%
14/03/23 13:43:55 INFO mapred.JobClient:  map 57% reduce 15%
14/03/23 13:44:04 INFO mapred.JobClient:  map 58% reduce 15%
14/03/23 13:44:16 INFO mapred.JobClient:  map 59% reduce 16%
14/03/23 13:44:28 INFO mapred.JobClient:  map 60% reduce 17%
14/03/23 13:44:34 INFO mapred.JobClient:  map 60% reduce 18%
14/03/23 13:44:37 INFO mapred.JobClient:  map 60% reduce 19%
14/03/23 13:44:43 INFO mapred.JobClient:  map 61% reduce 19%
14/03/23 13:44:55 INFO mapred.JobClient:  map 62% reduce 19%
14/03/23 13:45:07 INFO mapred.JobClient:  map 63% reduce 19%
14/03/23 13:45:16 INFO mapred.JobClient:  map 64% reduce 19%
14/03/23 13:45:25 INFO mapred.JobClient:  map 65% reduce 19%
14/03/23 13:45:37 INFO mapred.JobClient:  map 66% reduce 19%
14/03/23 13:45:46 INFO mapred.JobClient:  map 67% reduce 19%
14/03/23 13:45:55 INFO mapred.JobClient:  map 68% reduce 19%
14/03/23 13:46:07 INFO mapred.JobClient:  map 69% reduce 19%
14/03/23 13:46:16 INFO mapred.JobClient:  map 70% reduce 19%
14/03/23 13:46:25 INFO mapred.JobClient:  map 70% reduce 20%
14/03/23 13:46:31 INFO mapred.JobClient:  map 71% reduce 20%
14/03/23 13:46:37 INFO mapred.JobClient:  map 71% reduce 21%
14/03/23 13:46:43 INFO mapred.JobClient:  map 72% reduce 21%
14/03/23 13:46:52 INFO mapred.JobClient:  map 72% reduce 22%
14/03/23 13:46:55 INFO mapred.JobClient:  map 73% reduce 23%
14/03/23 13:47:07 INFO mapred.JobClient:  map 74% reduce 23%
14/03/23 13:47:19 INFO mapred.JobClient:  map 75% reduce 23%
14/03/23 13:47:28 INFO mapred.JobClient:  map 76% reduce 23%
14/03/23 13:47:40 INFO mapred.JobClient:  map 77% reduce 23%
14/03/23 13:47:49 INFO mapred.JobClient:  map 78% reduce 23%
14/03/23 13:47:58 INFO mapred.JobClient:  map 79% reduce 23%
14/03/23 13:48:10 INFO mapred.JobClient:  map 80% reduce 23%
14/03/23 13:48:19 INFO mapred.JobClient:  map 81% reduce 23%
14/03/23 13:48:31 INFO mapred.JobClient:  map 82% reduce 23%
14/03/23 13:48:40 INFO mapred.JobClient:  map 82% reduce 24%
14/03/23 13:48:43 INFO mapred.JobClient:  map 83% reduce 24%
14/03/23 13:48:49 INFO mapred.JobClient:  map 83% reduce 25%
14/03/23 13:48:55 INFO mapred.JobClient:  map 84% reduce 25%
14/03/23 13:49:04 INFO mapred.JobClient:  map 84% reduce 26%
14/03/23 13:49:10 INFO mapred.JobClient:  map 85% reduce 27%
14/03/23 13:49:19 INFO mapred.JobClient:  map 86% reduce 27%
14/03/23 13:49:31 INFO mapred.JobClient:  map 87% reduce 27%
14/03/23 13:49:41 INFO mapred.JobClient:  map 88% reduce 27%
14/03/23 13:49:50 INFO mapred.JobClient:  map 89% reduce 27%
14/03/23 13:49:59 INFO mapred.JobClient:  map 90% reduce 27%
14/03/23 13:50:11 INFO mapred.JobClient:  map 91% reduce 27%
14/03/23 13:50:20 INFO mapred.JobClient:  map 92% reduce 27%
14/03/23 13:50:29 INFO mapred.JobClient:  map 93% reduce 27%
14/03/23 13:50:41 INFO mapred.JobClient:  map 94% reduce 27%
14/03/23 13:50:50 INFO mapred.JobClient:  map 94% reduce 28%
14/03/23 13:50:53 INFO mapred.JobClient:  map 95% reduce 28%
14/03/23 13:50:59 INFO mapred.JobClient:  map 95% reduce 29%
14/03/23 13:51:05 INFO mapred.JobClient:  map 96% reduce 29%
14/03/23 13:51:08 INFO mapred.JobClient:  map 96% reduce 30%
14/03/23 13:51:23 INFO mapred.JobClient:  map 97% reduce 31%
14/03/23 13:51:50 INFO mapred.JobClient:  map 98% reduce 31%
14/03/23 13:52:20 INFO mapred.JobClient:  map 99% reduce 31%
14/03/23 13:52:53 INFO mapred.JobClient:  map 99% reduce 32%
14/03/23 13:52:59 INFO mapred.JobClient:  map 100% reduce 32%
14/03/23 13:53:08 INFO mapred.JobClient:  map 100% reduce 50%
14/03/23 13:53:11 INFO mapred.JobClient:  map 100% reduce 70%
14/03/23 13:53:14 INFO mapred.JobClient:  map 100% reduce 74%
14/03/23 13:53:17 INFO mapred.JobClient:  map 100% reduce 79%
14/03/23 13:53:20 INFO mapred.JobClient:  map 100% reduce 83%
14/03/23 13:53:23 INFO mapred.JobClient:  map 100% reduce 87%
14/03/23 13:53:26 INFO mapred.JobClient:  map 100% reduce 89%
14/03/23 13:53:29 INFO mapred.JobClient:  map 100% reduce 94%
14/03/23 13:53:32 INFO mapred.JobClient:  map 100% reduce 96%
14/03/23 13:53:35 INFO mapred.JobClient:  map 100% reduce 98%
14/03/23 13:53:44 INFO mapred.JobClient:  map 100% reduce 100%
14/03/23 13:53:49 INFO mapred.JobClient: Job complete: job_201403231329_0001
14/03/23 13:53:49 INFO mapred.JobClient: Counters: 29
14/03/23 13:53:49 INFO mapred.JobClient:   Job Counters 
14/03/23 13:53:49 INFO mapred.JobClient:     Launched reduce tasks=2
14/03/23 13:53:49 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=13327846
14/03/23 13:53:49 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
14/03/23 13:53:49 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
14/03/23 13:53:49 INFO mapred.JobClient:     Launched map tasks=100
14/03/23 13:53:49 INFO mapred.JobClient:     Data-local map tasks=100
14/03/23 13:53:49 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=2188378
14/03/23 13:53:49 INFO mapred.JobClient:   File Output Format Counters 
14/03/23 13:53:49 INFO mapred.JobClient:     Bytes Written=293004663
14/03/23 13:53:49 INFO mapred.JobClient:   FileSystemCounters
14/03/23 13:53:49 INFO mapred.JobClient:     FILE_BYTES_READ=1009300145
14/03/23 13:53:49 INFO mapred.JobClient:     HDFS_BYTES_READ=3118758693
14/03/23 13:53:49 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=2021293728
14/03/23 13:53:49 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=293004663
14/03/23 13:53:49 INFO mapred.JobClient:   File Input Format Counters 
14/03/23 13:53:49 INFO mapred.JobClient:     Bytes Read=3068323935
14/03/23 13:53:49 INFO mapred.JobClient:   Map-Reduce Framework
14/03/23 13:53:49 INFO mapred.JobClient:     Map output materialized bytes=1009301933
14/03/23 13:53:49 INFO mapred.JobClient:     Map input records=14303375
14/03/23 13:53:49 INFO mapred.JobClient:     Reduce shuffle bytes=1000092673
14/03/23 13:53:49 INFO mapred.JobClient:     Spilled Records=204800
14/03/23 13:53:49 INFO mapred.JobClient:     Map output bytes=1008902950
14/03/23 13:53:49 INFO mapred.JobClient:     CPU time spent (ms)=12993050
14/03/23 13:53:49 INFO mapred.JobClient:     Total committed heap usage (bytes)=228630200320
14/03/23 13:53:49 INFO mapred.JobClient:     Combine input records=0
14/03/23 13:53:49 INFO mapred.JobClient:     SPLIT_RAW_BYTES=13200
14/03/23 13:53:49 INFO mapred.JobClient:     Reduce input records=102400
14/03/23 13:53:49 INFO mapred.JobClient:     Reduce input groups=1024
14/03/23 13:53:49 INFO mapred.JobClient:     Combine output records=0
14/03/23 13:53:49 INFO mapred.JobClient:     Physical memory (bytes) snapshot=115650199552
14/03/23 13:53:49 INFO mapred.JobClient:     Reduce output records=1024
14/03/23 13:53:49 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=1969182973952
14/03/23 13:53:49 INFO mapred.JobClient:     Map output records=102400
14/03/23 13:53:49 INFO driver.MahoutDriver: Program took 1304907 ms (Minutes: 21.74845)

real	21m49.349s
user	0m10.478s
sys	0m2.757s
jmg3      5107  0.0  0.0 106084  1396 ?        Ss   13:53   0:00 bash -c ps aux | grep java
jmg3      5123  0.0  0.0 103232   844 ?        S    13:53   0:00 grep java
jmg3     29776  0.0  0.0  59072  3532 pts/0    S+   13:53   0:00 ssh -o ConnectTimeout=2 gpu-010 ps aux | grep java
jmg3     29782  0.0  0.0 106084  1400 ?        Ss   13:53   0:00 bash -c ps aux | grep java
jmg3     29798  0.0  0.0 103232   840 ?        S    13:53   0:00 grep java
java: no process killed
java: no process killed
jmg3      5169  0.0  0.0 106084  1404 ?        Ss   13:53   0:00 bash -c ps aux | grep java
jmg3      5185  0.0  0.0 103232   844 ?        S    13:53   0:00 grep java
jmg3     29860  0.0  0.0  59072  3528 pts/0    S+   13:53   0:00 ssh -o ConnectTimeout=2 gpu-010 ps aux | grep java
jmg3     29866  0.0  0.0 106084  1400 ?        Ss   13:53   0:00 bash -c ps aux | grep java
jmg3     29882  0.0  0.0 103232   844 ?        S    13:53   0:00 grep java
Setting path to /tmp/yiskylee
12 2 0 0 0 0 67108864 16
Max. num tasks = 12 mappers, 2 reducers
Mapper thread configs = GPU ( 0 ), CPU ( 0 )
Reducer thread configs = GPU ( 0 ), CPU ( 0 )
HDFS block size = 67108864
GPU/CPU Mults shouldn't matter



Here are the results:
export JAVA_HOME=/opt/apps/jdk/1.6
export HOME=/home/jmg3
export HADOOP_HOME=${HOME}/hadoop-1.0.3
export APARAPI_HOME=${HOME}/aparapi-read-only
export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true
export HADOOPCL_KERNEL_FOLDER=${HOME}/kernels
export HADOOP_LOG_DIR=/tmp/hadoop/logs
export HADOOP_APP_DIR=${HOME}/app
export CLASSPATH=${APARAPI_HOME}/com.amd.aparapi/dist/aparapi.jar:${HADOOP_HOME}/build/hadoop-core-1.0.4-SNAPSHOT.jar:${HOME}/hadoop-gpl-compression-read-only/build/hadoop-gpl-compression-0.2.0-dev.jar:${HADOOP_HOME}/build/hadoop-core-1.0.4-SNAPSHOT.jar:${HOME}/jfreechart-1.0.14.jar:.:${CLASSPATH}:/home/jmg3/mahout/math/target/mahout-math-0.9-SNAPSHOT.jar:/home/jmg3/mahout/integration/target/dependency/mahout-core-0.9-SNAPSHOT.jar
export HADOOP_CLASSPATH=${HOME}/hadoop-gpl-compression-read-only/build/hadoop-gpl-compression-0.2.0-dev.jar:${CLASSPATH}:${HADOOP_CLASSPATH}:/home/jmg3/mahout/math/target/mahout-math-0.9-SNAPSHOT.jar:/home/jmg3/mahout/integration/target/dependency/mahout-core-0.9-SNAPSHOT.jar
export JAVA_LIBRARY_PATH=${HOME}/lzo-install/lib:${JAVA_LIBRARY_PATH}

export SCIPY_HOME=${HOME}/scipy-install
export NUMPY_HOME=${HOME}/numpy-install
export SCIKIT_HOME=${HOME}/scikit-install
export PYTHONPATH=${SCIKIT_HOME}/lib64/python2.6/site-packages:${NUMPY_HOME}/lib64/python2.6/site-packages:${SCIPY_HOME}/lib64/python2.6/site-packages:${PYTHONPATH}

-----------------------------------------------------
  <name>mapred.job.tracker</name>
  <value>gpu-010.davinci.rice.edu:54311</value>
  <name>mapred.task.tracker.http.address</name>
  <value>0.0.0.0:50061</value>
  <name>mapred.reduce.parallel.copies</name><value>5</value>
  <name>tasktracker.http.threads</name><value>40</value>
  <name>mapred.map.tasks</name><value>24</value>
  <name>mapred.reduce.tasks</name><value>2</value>
  <name>mapred.tasktracker.map.tasks.maximum</name><value>12</value>
  <name>mapred.tasktracker.reduce.tasks.maximum</name><value>2</value>
  <name>mapred.map.tasks.speculative.execution</name>
  <value>true</value>
  <name>mapred.task.timeout</name><value>3600000</value>
  <!--name>mapred.task.timeout</name><value>1200000</value-->
  <name>mapred.child.java.opts</name><value>-Xms2G -Xmx16G -Dopencl.mapper.threadsPerGroup.gpu=0 -Dopencl.mapper.threadsPerGroup.cpu=0 -Dopencl.reducer.threadsPerGroup.gpu=0 -Dopencl.reducer.threadsPerGroup.cpu=0 -Dopencl.scheduler=BALANCED -Dopencl.vectorsToBuffer=32768</value>
  <name>mapred.map.tasks.speculative.execution</name><value>false</value>
  <name>mapred.reduce.tasks.speculative.execution</name><value>false</value>
  <name>mapreduce.map.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLBalanceScheduler</value>
  <!--name>mapreduce.map.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLML4Scheduler</value-->
  <name>mapreduce.reduce.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLBalanceScheduler</value>
  <!--name>mapreduce.reduce.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLML4Scheduler</value-->
  <name>io.compression.codecs</name>
  <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec</value>
  <name>io.sort.spill.percent</name>
  <value>0.7</value>
  <name>io.sort.spill.percent.hadoopcl</name>
  <value>0.6</value>
  <name>io.sort.spill.percent.hadoopcl.initial</name>
  <value>0.5</value>
  <name>io.sort.quickrestart</name>
  <value>0.4</value>
  <name>io.sort.maxrestart</name>
  <value>0.6</value>
  <name>io.sort.mb</name>
  <value>100</value>
  <name>min.num.spills.for.combine</name>
  <value>3</value>
  <name>opencl.global.buckets</name>
  <value>65536</value>
  <name>opencl.spill.chunk</name>
  <value>2</value>
<property><name>opencl.mapper.nkernels</name><value>1</value></property>
<property><name>opencl.mapper.ninputbuffers</name><value>2</value></property>
<property><name>opencl.mapper.noutputbuffers</name><value>4</value></property>
<property><name>opencl.mapper.prealloc.length.int</name><value>1966080</value></property>
<property><name>opencl.mapper.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.mapper.prealloc.length.double</name><value>1966080</value></property>
<property><name>opencl.mapper.inputBufferSize</name><value>65536</value></property>
<property><name>opencl.mapper.outputBufferSize</name><value>65536</value></property>
<property><name>opencl.mapper.val_ele_multiplier</name><value>15</value></property>
<!--property><name>opencl.mapper.compare</name><value>true</value></property-->
<property><name>opencl.combiner.nkernels</name><value>1</value></property>
<property><name>opencl.combiner.ninputbuffers</name><value>1</value></property>
<property><name>opencl.combiner.noutputbuffers</name><value>1</value></property>
<property><name>opencl.combiner.prealloc.length.int</name><value>7340032</value></property>
<property><name>opencl.combiner.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.combiner.prealloc.length.double</name><value>6291456</value></property>
<property><name>opencl.combiner.inputBufferSize</name><value>6000</value></property>
<property><name>opencl.combiner.outputBufferSize</name><value>16384</value></property>
<property><name>opencl.combiner.val_multiplier</name><value>60</value></property>
<property><name>opencl.combiner.val_ele_multiplier</name><value>20</value></property>
<property><name>opencl.reducer.nkernels</name><value>1</value></property>
<property><name>opencl.reducer.ninputbuffers</name><value>1</value></property>
<property><name>opencl.reducer.noutputbuffers</name><value>1</value></property>
<property><name>opencl.reducer.prealloc.length.int</name><value>16777216</value></property>
<property><name>opencl.reducer.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.reducer.prealloc.length.double</name><value>16777216</value></property>
<property><name>opencl.reducer.inputBufferSize</name><value>65536</value></property>
<property><name>opencl.reducer.outputBufferSize</name><value>2048</value></property>
  <name>opencl.buffer.diagnostics</name>
  <value>false</value>
  <name>opencl.profiling</name>
  <value>false</value>
  <name>opencl.highlevel</name>
  <value>false</value>
  <name>opencl.recordings_folder</name>
  <value>/scratch/jmg3/hadoopcl-recordings/</value>
-----------------------------------------------------
  <name>dfs.safemode.threshold.pct</name>
  <value>0</value>
  <name>dfs.replication</name><value>3</value>
  <name>dfs.block.size</name><value>67108864</value>
  <name>dfs.datanode.handler.count</name><value>3</value>
  <name>dfs.namenode.handler.count</name><value>10</value>
  <name>dfs.datanode.max.xcievers</name><value>256</value>
-----------------------------------------------------
  <name>hadoop.tmp.dir</name>
  <value>/tmp/hadoop/hadoop-${user.name}</value>
  <name>fs.default.name</name>
  <value>hdfs://gpu-010.davinci.rice.edu:54313</value>
<name>io.compression.codecs</name>
<value>
</value>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
-----------------------------------------------------
gpu-009
-----------------------------------------------------
gpu-010
-----------------------------------------------------
Completed reconfiguring
Warning: $HADOOP_HOME is deprecated.

14/03/23 13:54:00 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gpu-010.davinci.rice.edu/192.168.110.210
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 1.0.4-SNAPSHOT
STARTUP_MSG:   build =  -r ; compiled by 'jmg3' on Sun Mar 23 13:28:23 CDT 2014
************************************************************/
14/03/23 13:54:00 INFO util.GSet: VM type       = 64-bit
14/03/23 13:54:00 INFO util.GSet: 2% max memory = 17.77875 MB
14/03/23 13:54:00 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/03/23 13:54:00 INFO util.GSet: recommended=2097152, actual=2097152
14/03/23 13:54:00 INFO namenode.FSNamesystem: fsOwner=jmg3
14/03/23 13:54:01 INFO namenode.FSNamesystem: supergroup=supergroup
14/03/23 13:54:01 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/03/23 13:54:01 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/03/23 13:54:01 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/03/23 13:54:01 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/03/23 13:54:01 INFO common.Storage: Image file of size 110 saved in 0 seconds.
14/03/23 13:54:01 INFO common.Storage: Storage directory /tmp/hadoop/hadoop-jmg3/dfs/name has been successfully formatted.
14/03/23 13:54:01 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gpu-010.davinci.rice.edu/192.168.110.210
************************************************************/
Completed namenode startup
Warning: $HADOOP_HOME is deprecated.

no jobtracker to stop
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: no tasktracker to stop
no namenode to stop
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: no datanode to stop
gpu-010: Warning: $HADOOP_HOME is deprecated.
gpu-010: 
gpu-010: no secondarynamenode to stop
Completed stop all
Warning: $HADOOP_HOME is deprecated.

starting namenode, logging to /tmp/hadoop/logs/hadoop-jmg3-namenode-gpu-010.davinci.rice.edu.out
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: starting datanode, logging to /tmp/hadoop/logs/hadoop-jmg3-datanode-gpu-009.davinci.rice.edu.out
gpu-010: Warning: $HADOOP_HOME is deprecated.
gpu-010: 
gpu-010: starting secondarynamenode, logging to /tmp/hadoop/logs/hadoop-jmg3-secondarynamenode-gpu-010.davinci.rice.edu.out
starting jobtracker, logging to /tmp/hadoop/logs/hadoop-jmg3-jobtracker-gpu-010.davinci.rice.edu.out
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: starting tasktracker, logging to /tmp/hadoop/logs/hadoop-jmg3-tasktracker-gpu-009.davinci.rice.edu.out
Completed start all
Warning: $HADOOP_HOME is deprecated.

Warning: $HADOOP_HOME is deprecated.

Warning: $HADOOP_HOME is deprecated.

MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.
Warning: $HADOOP_HOME is deprecated.

Running on hadoop, using /home/jmg3/hadoop-1.0.3/bin/hadoop and HADOOP_CONF_DIR=/home/jmg3/hadoop-1.0.3/conf
MAHOUT-JOB: /home/jmg3/mahout/examples/target/mahout-examples-0.9-SNAPSHOT-job.jar
Warning: $HADOOP_HOME is deprecated.

14/03/23 13:58:15 INFO common.AbstractJob: Command line arguments: {--clusters=[clusters], --convergenceDelta=[0.5], --distanceMeasure=[org.apache.mahout.common.distance.CosineDistanceMeasure], --endPhase=[2147483647], --input=[input], --maxIter=[1], --method=[mapreduce], --output=[output], --overwrite=null, --startPhase=[0], --tempDir=[temp]}
14/03/23 13:58:16 INFO kmeans.KMeansDriver: Input: input Clusters In: clusters Out: output Distance: org.apache.mahout.common.distance.CosineDistanceMeasure
14/03/23 13:58:16 INFO kmeans.KMeansDriver: convergence: 0.5 max Iterations: 1
14/03/23 13:58:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/03/23 13:58:16 INFO compress.CodecPool: Got brand-new decompressor
Cluster Iterator running iteration 1/1 over priorPath: output/clusters-0
Sending 0 to HDFS took 32 ms
14/03/23 13:58:43 INFO input.FileInputFormat: Total input paths to process : 100
14/03/23 13:58:43 INFO mapred.JobClient: Running job: job_201403231354_0001
14/03/23 13:58:44 INFO mapred.JobClient:  map 0% reduce 0%
14/03/23 13:59:12 INFO mapred.JobClient:  map 1% reduce 0%
14/03/23 13:59:24 INFO mapred.JobClient:  map 2% reduce 0%
14/03/23 13:59:36 INFO mapred.JobClient:  map 3% reduce 0%
14/03/23 13:59:48 INFO mapred.JobClient:  map 4% reduce 0%
14/03/23 14:00:00 INFO mapred.JobClient:  map 5% reduce 0%
14/03/23 14:00:09 INFO mapred.JobClient:  map 6% reduce 0%
14/03/23 14:00:21 INFO mapred.JobClient:  map 7% reduce 0%
14/03/23 14:00:30 INFO mapred.JobClient:  map 8% reduce 0%
14/03/23 14:00:42 INFO mapred.JobClient:  map 9% reduce 0%
14/03/23 14:00:54 INFO mapred.JobClient:  map 10% reduce 0%
14/03/23 14:01:03 INFO mapred.JobClient:  map 11% reduce 0%
14/03/23 14:01:24 INFO mapred.JobClient:  map 12% reduce 2%
14/03/23 14:01:27 INFO mapred.JobClient:  map 12% reduce 3%
14/03/23 14:01:36 INFO mapred.JobClient:  map 13% reduce 3%
14/03/23 14:01:48 INFO mapred.JobClient:  map 14% reduce 3%
14/03/23 14:02:00 INFO mapred.JobClient:  map 15% reduce 3%
14/03/23 14:02:09 INFO mapred.JobClient:  map 16% reduce 3%
14/03/23 14:02:21 INFO mapred.JobClient:  map 17% reduce 3%
14/03/23 14:02:33 INFO mapred.JobClient:  map 18% reduce 3%
14/03/23 14:02:42 INFO mapred.JobClient:  map 19% reduce 3%
14/03/23 14:02:54 INFO mapred.JobClient:  map 20% reduce 3%
14/03/23 14:03:03 INFO mapred.JobClient:  map 21% reduce 3%
14/03/23 14:03:15 INFO mapred.JobClient:  map 22% reduce 3%
14/03/23 14:03:24 INFO mapred.JobClient:  map 23% reduce 3%
14/03/23 14:03:33 INFO mapred.JobClient:  map 23% reduce 4%
14/03/23 14:03:39 INFO mapred.JobClient:  map 23% reduce 6%
14/03/23 14:03:42 INFO mapred.JobClient:  map 24% reduce 6%
14/03/23 14:03:48 INFO mapred.JobClient:  map 24% reduce 7%
14/03/23 14:03:57 INFO mapred.JobClient:  map 25% reduce 7%
14/03/23 14:04:10 INFO mapred.JobClient:  map 26% reduce 7%
14/03/23 14:04:19 INFO mapred.JobClient:  map 27% reduce 7%
14/03/23 14:04:31 INFO mapred.JobClient:  map 28% reduce 7%
14/03/23 14:04:40 INFO mapred.JobClient:  map 29% reduce 7%
14/03/23 14:04:49 INFO mapred.JobClient:  map 30% reduce 7%
14/03/23 14:05:01 INFO mapred.JobClient:  map 31% reduce 7%
14/03/23 14:05:10 INFO mapred.JobClient:  map 32% reduce 7%
14/03/23 14:05:22 INFO mapred.JobClient:  map 33% reduce 7%
14/03/23 14:05:31 INFO mapred.JobClient:  map 34% reduce 7%
14/03/23 14:05:43 INFO mapred.JobClient:  map 35% reduce 7%
14/03/23 14:05:52 INFO mapred.JobClient:  map 35% reduce 8%
14/03/23 14:05:58 INFO mapred.JobClient:  map 36% reduce 9%
14/03/23 14:06:04 INFO mapred.JobClient:  map 36% reduce 10%
14/03/23 14:06:10 INFO mapred.JobClient:  map 36% reduce 11%
14/03/23 14:06:13 INFO mapred.JobClient:  map 37% reduce 11%
14/03/23 14:06:25 INFO mapred.JobClient:  map 38% reduce 11%
14/03/23 14:06:37 INFO mapred.JobClient:  map 39% reduce 11%
14/03/23 14:06:46 INFO mapred.JobClient:  map 40% reduce 11%
14/03/23 14:06:55 INFO mapred.JobClient:  map 41% reduce 11%
14/03/23 14:07:07 INFO mapred.JobClient:  map 42% reduce 11%
14/03/23 14:07:16 INFO mapred.JobClient:  map 43% reduce 11%
14/03/23 14:07:28 INFO mapred.JobClient:  map 44% reduce 11%
14/03/23 14:07:40 INFO mapred.JobClient:  map 45% reduce 11%
14/03/23 14:07:49 INFO mapred.JobClient:  map 46% reduce 11%
14/03/23 14:07:58 INFO mapred.JobClient:  map 46% reduce 12%
14/03/23 14:08:01 INFO mapred.JobClient:  map 47% reduce 12%
14/03/23 14:08:10 INFO mapred.JobClient:  map 47% reduce 13%
14/03/23 14:08:16 INFO mapred.JobClient:  map 48% reduce 13%
14/03/23 14:08:19 INFO mapred.JobClient:  map 48% reduce 14%
14/03/23 14:08:28 INFO mapred.JobClient:  map 49% reduce 15%
14/03/23 14:08:40 INFO mapred.JobClient:  map 50% reduce 15%
14/03/23 14:08:52 INFO mapred.JobClient:  map 51% reduce 15%
14/03/23 14:09:01 INFO mapred.JobClient:  map 52% reduce 15%
14/03/23 14:09:10 INFO mapred.JobClient:  map 53% reduce 15%
14/03/23 14:09:22 INFO mapred.JobClient:  map 54% reduce 15%
14/03/23 14:09:31 INFO mapred.JobClient:  map 55% reduce 15%
14/03/23 14:09:43 INFO mapred.JobClient:  map 56% reduce 15%
14/03/23 14:09:52 INFO mapred.JobClient:  map 57% reduce 15%
14/03/23 14:10:04 INFO mapred.JobClient:  map 58% reduce 16%
14/03/23 14:10:16 INFO mapred.JobClient:  map 59% reduce 16%
14/03/23 14:10:19 INFO mapred.JobClient:  map 59% reduce 17%
14/03/23 14:10:28 INFO mapred.JobClient:  map 60% reduce 17%
14/03/23 14:10:34 INFO mapred.JobClient:  map 60% reduce 18%
14/03/23 14:10:40 INFO mapred.JobClient:  map 61% reduce 18%
14/03/23 14:10:46 INFO mapred.JobClient:  map 61% reduce 19%
14/03/23 14:10:52 INFO mapred.JobClient:  map 62% reduce 19%
14/03/23 14:11:04 INFO mapred.JobClient:  map 63% reduce 19%
14/03/23 14:11:13 INFO mapred.JobClient:  map 64% reduce 19%
14/03/23 14:11:25 INFO mapred.JobClient:  map 65% reduce 19%
14/03/23 14:11:34 INFO mapred.JobClient:  map 66% reduce 19%
14/03/23 14:11:43 INFO mapred.JobClient:  map 67% reduce 19%
14/03/23 14:11:52 INFO mapred.JobClient:  map 68% reduce 19%
14/03/23 14:12:04 INFO mapred.JobClient:  map 69% reduce 19%
14/03/23 14:12:10 INFO mapred.JobClient:  map 69% reduce 20%
14/03/23 14:12:13 INFO mapred.JobClient:  map 70% reduce 20%
14/03/23 14:12:28 INFO mapred.JobClient:  map 71% reduce 20%
14/03/23 14:12:31 INFO mapred.JobClient:  map 71% reduce 21%
14/03/23 14:12:40 INFO mapred.JobClient:  map 72% reduce 21%
14/03/23 14:12:46 INFO mapred.JobClient:  map 72% reduce 22%
14/03/23 14:12:52 INFO mapred.JobClient:  map 73% reduce 22%
14/03/23 14:13:01 INFO mapred.JobClient:  map 73% reduce 23%
14/03/23 14:13:04 INFO mapred.JobClient:  map 74% reduce 23%
14/03/23 14:13:16 INFO mapred.JobClient:  map 75% reduce 23%
14/03/23 14:13:25 INFO mapred.JobClient:  map 76% reduce 23%
14/03/23 14:13:34 INFO mapred.JobClient:  map 77% reduce 23%
14/03/23 14:13:46 INFO mapred.JobClient:  map 78% reduce 23%
14/03/23 14:13:55 INFO mapred.JobClient:  map 79% reduce 23%
14/03/23 14:14:05 INFO mapred.JobClient:  map 80% reduce 23%
14/03/23 14:14:17 INFO mapred.JobClient:  map 81% reduce 23%
14/03/23 14:14:20 INFO mapred.JobClient:  map 81% reduce 24%
14/03/23 14:14:29 INFO mapred.JobClient:  map 82% reduce 24%
14/03/23 14:14:41 INFO mapred.JobClient:  map 83% reduce 25%
14/03/23 14:14:53 INFO mapred.JobClient:  map 84% reduce 25%
14/03/23 14:14:56 INFO mapred.JobClient:  map 84% reduce 26%
14/03/23 14:15:05 INFO mapred.JobClient:  map 85% reduce 26%
14/03/23 14:15:11 INFO mapred.JobClient:  map 85% reduce 27%
14/03/23 14:15:17 INFO mapred.JobClient:  map 86% reduce 27%
14/03/23 14:15:26 INFO mapred.JobClient:  map 87% reduce 27%
14/03/23 14:15:38 INFO mapred.JobClient:  map 88% reduce 27%
14/03/23 14:15:47 INFO mapred.JobClient:  map 89% reduce 27%
14/03/23 14:15:56 INFO mapred.JobClient:  map 90% reduce 27%
14/03/23 14:16:05 INFO mapred.JobClient:  map 91% reduce 27%
14/03/23 14:16:14 INFO mapred.JobClient:  map 92% reduce 27%
14/03/23 14:16:26 INFO mapred.JobClient:  map 93% reduce 27%
14/03/23 14:16:29 INFO mapred.JobClient:  map 93% reduce 28%
14/03/23 14:16:35 INFO mapred.JobClient:  map 94% reduce 28%
14/03/23 14:16:47 INFO mapred.JobClient:  map 95% reduce 28%
14/03/23 14:16:50 INFO mapred.JobClient:  map 95% reduce 29%
14/03/23 14:16:59 INFO mapred.JobClient:  map 96% reduce 29%
14/03/23 14:17:02 INFO mapred.JobClient:  map 96% reduce 30%
14/03/23 14:17:14 INFO mapred.JobClient:  map 97% reduce 30%
14/03/23 14:17:20 INFO mapred.JobClient:  map 97% reduce 31%
14/03/23 14:17:38 INFO mapred.JobClient:  map 98% reduce 31%
14/03/23 14:18:05 INFO mapred.JobClient:  map 99% reduce 31%
14/03/23 14:18:35 INFO mapred.JobClient:  map 99% reduce 32%
14/03/23 14:18:41 INFO mapred.JobClient:  map 100% reduce 32%
14/03/23 14:18:50 INFO mapred.JobClient:  map 100% reduce 50%
14/03/23 14:18:53 INFO mapred.JobClient:  map 100% reduce 67%
14/03/23 14:18:56 INFO mapred.JobClient:  map 100% reduce 72%
14/03/23 14:18:59 INFO mapred.JobClient:  map 100% reduce 76%
14/03/23 14:19:02 INFO mapred.JobClient:  map 100% reduce 80%
14/03/23 14:19:05 INFO mapred.JobClient:  map 100% reduce 84%
14/03/23 14:19:08 INFO mapred.JobClient:  map 100% reduce 89%
14/03/23 14:19:11 INFO mapred.JobClient:  map 100% reduce 91%
14/03/23 14:19:14 INFO mapred.JobClient:  map 100% reduce 97%
14/03/23 14:19:20 INFO mapred.JobClient:  map 100% reduce 100%
14/03/23 14:19:25 INFO mapred.JobClient: Job complete: job_201403231354_0001
14/03/23 14:19:25 INFO mapred.JobClient: Counters: 29
14/03/23 14:19:25 INFO mapred.JobClient:   Job Counters 
14/03/23 14:19:25 INFO mapred.JobClient:     Launched reduce tasks=2
14/03/23 14:19:25 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=13295398
14/03/23 14:19:25 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
14/03/23 14:19:25 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
14/03/23 14:19:25 INFO mapred.JobClient:     Launched map tasks=100
14/03/23 14:19:25 INFO mapred.JobClient:     Data-local map tasks=100
14/03/23 14:19:25 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=2160390
14/03/23 14:19:25 INFO mapred.JobClient:   File Output Format Counters 
14/03/23 14:19:25 INFO mapred.JobClient:     Bytes Written=293004675
14/03/23 14:19:25 INFO mapred.JobClient:   FileSystemCounters
14/03/23 14:19:25 INFO mapred.JobClient:     FILE_BYTES_READ=1009300145
14/03/23 14:19:25 INFO mapred.JobClient:     HDFS_BYTES_READ=3118758693
14/03/23 14:19:25 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=2021293728
14/03/23 14:19:25 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=293004675
14/03/23 14:19:25 INFO mapred.JobClient:   File Input Format Counters 
14/03/23 14:19:25 INFO mapred.JobClient:     Bytes Read=3068323935
14/03/23 14:19:25 INFO mapred.JobClient:   Map-Reduce Framework
14/03/23 14:19:25 INFO mapred.JobClient:     Map output materialized bytes=1009301933
14/03/23 14:19:25 INFO mapred.JobClient:     Map input records=14303375
14/03/23 14:19:25 INFO mapred.JobClient:     Reduce shuffle bytes=1000092673
14/03/23 14:19:25 INFO mapred.JobClient:     Spilled Records=204800
14/03/23 14:19:25 INFO mapred.JobClient:     Map output bytes=1008902950
14/03/23 14:19:25 INFO mapred.JobClient:     CPU time spent (ms)=12934270
14/03/23 14:19:25 INFO mapred.JobClient:     Total committed heap usage (bytes)=227192995840
14/03/23 14:19:25 INFO mapred.JobClient:     Combine input records=0
14/03/23 14:19:25 INFO mapred.JobClient:     SPLIT_RAW_BYTES=13200
14/03/23 14:19:25 INFO mapred.JobClient:     Reduce input records=102400
14/03/23 14:19:25 INFO mapred.JobClient:     Reduce input groups=1024
14/03/23 14:19:25 INFO mapred.JobClient:     Combine output records=0
14/03/23 14:19:25 INFO mapred.JobClient:     Physical memory (bytes) snapshot=115087306752
14/03/23 14:19:25 INFO mapred.JobClient:     Reduce output records=1024
14/03/23 14:19:25 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=1968781373440
14/03/23 14:19:25 INFO mapred.JobClient:     Map output records=102400
14/03/23 14:19:25 INFO driver.MahoutDriver: Program took 1269759 ms (Minutes: 21.16265)

real	21m14.188s
user	0m10.563s
sys	0m2.787s

jmg3     25668  0.0  0.0 106084  1400 ?        Ss   14:27   0:00 bash -c ps aux | grep java
jmg3     25684  0.0  0.0 103232   844 ?        S    14:27   0:00 grep java
jmg3       931  0.0  0.0  59072  3528 pts/0    S+   14:27   0:00 ssh -o ConnectTimeout=2 gpu-010 ps aux | grep java
jmg3       937  0.0  0.0 106084  1396 ?        Ss   14:27   0:00 bash -c ps aux | grep java
jmg3       953  0.0  0.0 103232   836 ?        S    14:27   0:00 grep java
java: no process killed
java: no process killed
jmg3     25730  0.0  0.0 106084  1400 ?        Ss   14:27   0:00 bash -c ps aux | grep java
jmg3     25746  0.0  0.0 103232   840 ?        S    14:27   0:00 grep java
jmg3      1015  0.0  0.0  59072  3528 pts/0    S+   14:27   0:00 ssh -o ConnectTimeout=2 gpu-010 ps aux | grep java
jmg3      1021  0.0  0.0 106084  1396 ?        Ss   14:27   0:00 bash -c ps aux | grep java
jmg3      1037  0.0  0.0 103232   836 ?        S    14:27   0:00 grep java
Setting path to /tmp/yiskylee
12 2 0 0 0 0 67108864 16
Max. num tasks = 12 mappers, 2 reducers
Mapper thread configs = GPU ( 0 ), CPU ( 0 )
Reducer thread configs = GPU ( 0 ), CPU ( 0 )
HDFS block size = 67108864
GPU/CPU Mults shouldn't matter



Here are the results:
export JAVA_HOME=/opt/apps/jdk/1.6
export HOME=/home/jmg3
export HADOOP_HOME=${HOME}/hadoop-1.0.3
export APARAPI_HOME=${HOME}/aparapi-read-only
export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true
export HADOOPCL_KERNEL_FOLDER=${HOME}/kernels
export HADOOP_LOG_DIR=/tmp/hadoop/logs
export HADOOP_APP_DIR=${HOME}/app
export CLASSPATH=${APARAPI_HOME}/com.amd.aparapi/dist/aparapi.jar:${HADOOP_HOME}/build/hadoop-core-1.0.4-SNAPSHOT.jar:${HOME}/hadoop-gpl-compression-read-only/build/hadoop-gpl-compression-0.2.0-dev.jar:${HADOOP_HOME}/build/hadoop-core-1.0.4-SNAPSHOT.jar:${HOME}/jfreechart-1.0.14.jar:.:${CLASSPATH}:/home/jmg3/mahout/math/target/mahout-math-0.9-SNAPSHOT.jar:/home/jmg3/mahout/integration/target/dependency/mahout-core-0.9-SNAPSHOT.jar
export HADOOP_CLASSPATH=${HOME}/hadoop-gpl-compression-read-only/build/hadoop-gpl-compression-0.2.0-dev.jar:${CLASSPATH}:${HADOOP_CLASSPATH}:/home/jmg3/mahout/math/target/mahout-math-0.9-SNAPSHOT.jar:/home/jmg3/mahout/integration/target/dependency/mahout-core-0.9-SNAPSHOT.jar
export JAVA_LIBRARY_PATH=${HOME}/lzo-install/lib:${JAVA_LIBRARY_PATH}

export SCIPY_HOME=${HOME}/scipy-install
export NUMPY_HOME=${HOME}/numpy-install
export SCIKIT_HOME=${HOME}/scikit-install
export PYTHONPATH=${SCIKIT_HOME}/lib64/python2.6/site-packages:${NUMPY_HOME}/lib64/python2.6/site-packages:${SCIPY_HOME}/lib64/python2.6/site-packages:${PYTHONPATH}

-----------------------------------------------------
  <name>mapred.job.tracker</name>
  <value>gpu-010.davinci.rice.edu:54311</value>
  <name>mapred.task.tracker.http.address</name>
  <value>0.0.0.0:50061</value>
  <name>mapred.reduce.parallel.copies</name><value>5</value>
  <name>tasktracker.http.threads</name><value>40</value>
  <name>mapred.map.tasks</name><value>24</value>
  <name>mapred.reduce.tasks</name><value>2</value>
  <name>mapred.tasktracker.map.tasks.maximum</name><value>12</value>
  <name>mapred.tasktracker.reduce.tasks.maximum</name><value>2</value>
  <name>mapred.map.tasks.speculative.execution</name>
  <value>true</value>
  <name>mapred.task.timeout</name><value>3600000</value>
  <!--name>mapred.task.timeout</name><value>1200000</value-->
  <name>mapred.child.java.opts</name><value>-Xms2G -Xmx16G -Dopencl.mapper.threadsPerGroup.gpu=0 -Dopencl.mapper.threadsPerGroup.cpu=0 -Dopencl.reducer.threadsPerGroup.gpu=0 -Dopencl.reducer.threadsPerGroup.cpu=0 -Dopencl.scheduler=BALANCED -Dopencl.vectorsToBuffer=32768</value>
  <name>mapred.map.tasks.speculative.execution</name><value>false</value>
  <name>mapred.reduce.tasks.speculative.execution</name><value>false</value>
  <name>mapreduce.map.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLBalanceScheduler</value>
  <!--name>mapreduce.map.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLML4Scheduler</value-->
  <name>mapreduce.reduce.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLBalanceScheduler</value>
  <!--name>mapreduce.reduce.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLML4Scheduler</value-->
  <name>io.compression.codecs</name>
  <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec</value>
  <name>io.sort.spill.percent</name>
  <value>0.7</value>
  <name>io.sort.spill.percent.hadoopcl</name>
  <value>0.6</value>
  <name>io.sort.spill.percent.hadoopcl.initial</name>
  <value>0.5</value>
  <name>io.sort.quickrestart</name>
  <value>0.4</value>
  <name>io.sort.maxrestart</name>
  <value>0.6</value>
  <name>io.sort.mb</name>
  <value>100</value>
  <name>min.num.spills.for.combine</name>
  <value>3</value>
  <name>opencl.global.buckets</name>
  <value>65536</value>
  <name>opencl.spill.chunk</name>
  <value>2</value>
<property><name>opencl.mapper.nkernels</name><value>1</value></property>
<property><name>opencl.mapper.ninputbuffers</name><value>2</value></property>
<property><name>opencl.mapper.noutputbuffers</name><value>4</value></property>
<property><name>opencl.mapper.prealloc.length.int</name><value>1966080</value></property>
<property><name>opencl.mapper.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.mapper.prealloc.length.double</name><value>1966080</value></property>
<property><name>opencl.mapper.inputBufferSize</name><value>65536</value></property>
<property><name>opencl.mapper.outputBufferSize</name><value>65536</value></property>
<property><name>opencl.mapper.val_ele_multiplier</name><value>15</value></property>
<!--property><name>opencl.mapper.compare</name><value>true</value></property-->
<property><name>opencl.combiner.nkernels</name><value>1</value></property>
<property><name>opencl.combiner.ninputbuffers</name><value>1</value></property>
<property><name>opencl.combiner.noutputbuffers</name><value>1</value></property>
<property><name>opencl.combiner.prealloc.length.int</name><value>7340032</value></property>
<property><name>opencl.combiner.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.combiner.prealloc.length.double</name><value>6291456</value></property>
<property><name>opencl.combiner.inputBufferSize</name><value>6000</value></property>
<property><name>opencl.combiner.outputBufferSize</name><value>16384</value></property>
<property><name>opencl.combiner.val_multiplier</name><value>60</value></property>
<property><name>opencl.combiner.val_ele_multiplier</name><value>20</value></property>
<property><name>opencl.reducer.nkernels</name><value>1</value></property>
<property><name>opencl.reducer.ninputbuffers</name><value>1</value></property>
<property><name>opencl.reducer.noutputbuffers</name><value>1</value></property>
<property><name>opencl.reducer.prealloc.length.int</name><value>16777216</value></property>
<property><name>opencl.reducer.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.reducer.prealloc.length.double</name><value>16777216</value></property>
<property><name>opencl.reducer.inputBufferSize</name><value>65536</value></property>
<property><name>opencl.reducer.outputBufferSize</name><value>2048</value></property>
  <name>opencl.buffer.diagnostics</name>
  <value>false</value>
  <name>opencl.profiling</name>
  <value>false</value>
  <name>opencl.highlevel</name>
  <value>false</value>
  <name>opencl.recordings_folder</name>
  <value>/scratch/jmg3/hadoopcl-recordings/</value>
-----------------------------------------------------
  <name>dfs.safemode.threshold.pct</name>
  <value>0</value>
  <name>dfs.replication</name><value>3</value>
  <name>dfs.block.size</name><value>67108864</value>
  <name>dfs.datanode.handler.count</name><value>3</value>
  <name>dfs.namenode.handler.count</name><value>10</value>
  <name>dfs.datanode.max.xcievers</name><value>256</value>
-----------------------------------------------------
  <name>hadoop.tmp.dir</name>
  <value>/tmp/hadoop/hadoop-${user.name}</value>
  <name>fs.default.name</name>
  <value>hdfs://gpu-010.davinci.rice.edu:54313</value>
<name>io.compression.codecs</name>
<value>
</value>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
-----------------------------------------------------
gpu-009
-----------------------------------------------------
gpu-010
-----------------------------------------------------
Completed reconfiguring
Warning: $HADOOP_HOME is deprecated.

14/03/23 14:27:14 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gpu-010.davinci.rice.edu/192.168.110.210
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 1.0.4-SNAPSHOT
STARTUP_MSG:   build =  -r ; compiled by 'jmg3' on Sun Mar 23 13:28:23 CDT 2014
************************************************************/
14/03/23 14:27:14 INFO util.GSet: VM type       = 64-bit
14/03/23 14:27:14 INFO util.GSet: 2% max memory = 17.77875 MB
14/03/23 14:27:14 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/03/23 14:27:14 INFO util.GSet: recommended=2097152, actual=2097152
14/03/23 14:27:14 INFO namenode.FSNamesystem: fsOwner=jmg3
14/03/23 14:27:14 INFO namenode.FSNamesystem: supergroup=supergroup
14/03/23 14:27:14 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/03/23 14:27:14 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/03/23 14:27:14 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/03/23 14:27:14 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/03/23 14:27:14 INFO common.Storage: Image file of size 110 saved in 0 seconds.
14/03/23 14:27:14 INFO common.Storage: Storage directory /tmp/hadoop/hadoop-jmg3/dfs/name has been successfully formatted.
14/03/23 14:27:14 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gpu-010.davinci.rice.edu/192.168.110.210
************************************************************/
Completed namenode startup
Warning: $HADOOP_HOME is deprecated.

no jobtracker to stop
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: no tasktracker to stop
no namenode to stop
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: no datanode to stop
gpu-010: Warning: $HADOOP_HOME is deprecated.
gpu-010: 
gpu-010: no secondarynamenode to stop
Completed stop all
Warning: $HADOOP_HOME is deprecated.

starting namenode, logging to /tmp/hadoop/logs/hadoop-jmg3-namenode-gpu-010.davinci.rice.edu.out
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: starting datanode, logging to /tmp/hadoop/logs/hadoop-jmg3-datanode-gpu-009.davinci.rice.edu.out
gpu-010: Warning: $HADOOP_HOME is deprecated.
gpu-010: 
gpu-010: starting secondarynamenode, logging to /tmp/hadoop/logs/hadoop-jmg3-secondarynamenode-gpu-010.davinci.rice.edu.out
starting jobtracker, logging to /tmp/hadoop/logs/hadoop-jmg3-jobtracker-gpu-010.davinci.rice.edu.out
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: starting tasktracker, logging to /tmp/hadoop/logs/hadoop-jmg3-tasktracker-gpu-009.davinci.rice.edu.out
Completed start all
Warning: $HADOOP_HOME is deprecated.

Warning: $HADOOP_HOME is deprecated.

Warning: $HADOOP_HOME is deprecated.

MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.
Warning: $HADOOP_HOME is deprecated.

Running on hadoop, using /home/jmg3/hadoop-1.0.3/bin/hadoop and HADOOP_CONF_DIR=/home/jmg3/hadoop-1.0.3/conf
MAHOUT-JOB: /home/jmg3/mahout/examples/target/mahout-examples-0.9-SNAPSHOT-job.jar
Warning: $HADOOP_HOME is deprecated.

14/03/23 14:29:35 INFO common.AbstractJob: Command line arguments: {--clusters=[clusters], --convergenceDelta=[0.5], --distanceMeasure=[org.apache.mahout.common.distance.CosineDistanceMeasure], --endPhase=[2147483647], --input=[input], --maxIter=[1], --method=[mapreduce], --output=[output], --overwrite=null, --startPhase=[0], --tempDir=[temp]}
14/03/23 14:29:36 INFO kmeans.KMeansDriver: Input: input Clusters In: clusters Out: output Distance: org.apache.mahout.common.distance.CosineDistanceMeasure
14/03/23 14:29:36 INFO kmeans.KMeansDriver: convergence: 0.5 max Iterations: 1
14/03/23 14:29:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/03/23 14:29:36 INFO compress.CodecPool: Got brand-new decompressor
Cluster Iterator running iteration 1/1 over priorPath: output/clusters-0
Sending 0 to HDFS took 192 ms
14/03/23 14:30:12 INFO input.FileInputFormat: Total input paths to process : 100
14/03/23 14:30:12 INFO mapred.JobClient: Running job: job_201403231427_0001
14/03/23 14:30:13 INFO mapred.JobClient:  map 0% reduce 0%
14/03/23 14:30:47 INFO mapred.JobClient:  map 1% reduce 0%
14/03/23 14:30:59 INFO mapred.JobClient:  map 2% reduce 0%
14/03/23 14:31:11 INFO mapred.JobClient:  map 3% reduce 0%
14/03/23 14:31:19 INFO mapred.JobClient:  map 4% reduce 0%
14/03/23 14:31:34 INFO mapred.JobClient:  map 5% reduce 0%
14/03/23 14:31:43 INFO mapred.JobClient:  map 6% reduce 0%
14/03/23 14:31:55 INFO mapred.JobClient:  map 7% reduce 0%
14/03/23 14:32:07 INFO mapred.JobClient:  map 8% reduce 0%
14/03/23 14:32:16 INFO mapred.JobClient:  map 9% reduce 0%
14/03/23 14:32:28 INFO mapred.JobClient:  map 10% reduce 0%
14/03/23 14:32:40 INFO mapred.JobClient:  map 11% reduce 0%
14/03/23 14:32:58 INFO mapred.JobClient:  map 12% reduce 2%
14/03/23 14:33:07 INFO mapred.JobClient:  map 12% reduce 3%
14/03/23 14:33:10 INFO mapred.JobClient:  map 13% reduce 3%
14/03/23 14:33:22 INFO mapred.JobClient:  map 14% reduce 3%
14/03/23 14:33:34 INFO mapred.JobClient:  map 15% reduce 3%
14/03/23 14:33:43 INFO mapred.JobClient:  map 16% reduce 3%
14/03/23 14:33:55 INFO mapred.JobClient:  map 17% reduce 3%
14/03/23 14:34:07 INFO mapred.JobClient:  map 18% reduce 3%
14/03/23 14:34:19 INFO mapred.JobClient:  map 19% reduce 3%
14/03/23 14:34:28 INFO mapred.JobClient:  map 20% reduce 3%
14/03/23 14:34:40 INFO mapred.JobClient:  map 21% reduce 3%
14/03/23 14:34:49 INFO mapred.JobClient:  map 22% reduce 3%
14/03/23 14:34:58 INFO mapred.JobClient:  map 23% reduce 3%
14/03/23 14:35:07 INFO mapred.JobClient:  map 23% reduce 4%
14/03/23 14:35:13 INFO mapred.JobClient:  map 23% reduce 5%
14/03/23 14:35:16 INFO mapred.JobClient:  map 24% reduce 5%
14/03/23 14:35:22 INFO mapred.JobClient:  map 24% reduce 6%
14/03/23 14:35:25 INFO mapred.JobClient:  map 24% reduce 7%
14/03/23 14:35:31 INFO mapred.JobClient:  map 25% reduce 7%
14/03/23 14:35:40 INFO mapred.JobClient:  map 26% reduce 7%
14/03/23 14:35:52 INFO mapred.JobClient:  map 27% reduce 7%
14/03/23 14:36:01 INFO mapred.JobClient:  map 28% reduce 7%
14/03/23 14:36:13 INFO mapred.JobClient:  map 29% reduce 7%
14/03/23 14:36:22 INFO mapred.JobClient:  map 30% reduce 7%
14/03/23 14:36:34 INFO mapred.JobClient:  map 31% reduce 7%
14/03/23 14:36:43 INFO mapred.JobClient:  map 32% reduce 7%
14/03/23 14:36:55 INFO mapred.JobClient:  map 33% reduce 7%
14/03/23 14:37:04 INFO mapred.JobClient:  map 34% reduce 7%
14/03/23 14:37:16 INFO mapred.JobClient:  map 35% reduce 7%
14/03/23 14:37:19 INFO mapred.JobClient:  map 35% reduce 8%
14/03/23 14:37:25 INFO mapred.JobClient:  map 35% reduce 9%
14/03/23 14:37:31 INFO mapred.JobClient:  map 36% reduce 9%
14/03/23 14:37:40 INFO mapred.JobClient:  map 36% reduce 10%
14/03/23 14:37:46 INFO mapred.JobClient:  map 37% reduce 10%
14/03/23 14:37:49 INFO mapred.JobClient:  map 37% reduce 11%
14/03/23 14:37:58 INFO mapred.JobClient:  map 38% reduce 11%
14/03/23 14:38:07 INFO mapred.JobClient:  map 39% reduce 11%
14/03/23 14:38:16 INFO mapred.JobClient:  map 40% reduce 11%
14/03/23 14:38:28 INFO mapred.JobClient:  map 41% reduce 11%
14/03/23 14:38:37 INFO mapred.JobClient:  map 42% reduce 11%
14/03/23 14:38:46 INFO mapred.JobClient:  map 43% reduce 11%
14/03/23 14:38:58 INFO mapred.JobClient:  map 44% reduce 11%
14/03/23 14:39:07 INFO mapred.JobClient:  map 45% reduce 11%
14/03/23 14:39:19 INFO mapred.JobClient:  map 46% reduce 11%
14/03/23 14:39:25 INFO mapred.JobClient:  map 46% reduce 12%
14/03/23 14:39:31 INFO mapred.JobClient:  map 47% reduce 12%
14/03/23 14:39:34 INFO mapred.JobClient:  map 47% reduce 13%
14/03/23 14:39:43 INFO mapred.JobClient:  map 48% reduce 13%
14/03/23 14:39:56 INFO mapred.JobClient:  map 48% reduce 14%
14/03/23 14:39:59 INFO mapred.JobClient:  map 49% reduce 14%
14/03/23 14:40:05 INFO mapred.JobClient:  map 49% reduce 15%
14/03/23 14:40:11 INFO mapred.JobClient:  map 50% reduce 15%
14/03/23 14:40:20 INFO mapred.JobClient:  map 51% reduce 15%
14/03/23 14:40:32 INFO mapred.JobClient:  map 52% reduce 15%
14/03/23 14:40:41 INFO mapred.JobClient:  map 53% reduce 15%
14/03/23 14:40:53 INFO mapred.JobClient:  map 54% reduce 15%
14/03/23 14:41:02 INFO mapred.JobClient:  map 55% reduce 15%
14/03/23 14:41:14 INFO mapred.JobClient:  map 56% reduce 15%
14/03/23 14:41:23 INFO mapred.JobClient:  map 57% reduce 15%
14/03/23 14:41:35 INFO mapred.JobClient:  map 58% reduce 15%
14/03/23 14:41:41 INFO mapred.JobClient:  map 58% reduce 16%
14/03/23 14:41:47 INFO mapred.JobClient:  map 59% reduce 16%
14/03/23 14:41:50 INFO mapred.JobClient:  map 59% reduce 17%
14/03/23 14:41:59 INFO mapred.JobClient:  map 60% reduce 17%
14/03/23 14:42:11 INFO mapred.JobClient:  map 61% reduce 19%
14/03/23 14:42:23 INFO mapred.JobClient:  map 62% reduce 19%
14/03/23 14:42:35 INFO mapred.JobClient:  map 63% reduce 19%
14/03/23 14:42:44 INFO mapred.JobClient:  map 64% reduce 19%
14/03/23 14:42:56 INFO mapred.JobClient:  map 65% reduce 19%
14/03/23 14:43:05 INFO mapred.JobClient:  map 66% reduce 19%
14/03/23 14:43:14 INFO mapred.JobClient:  map 67% reduce 19%
14/03/23 14:43:26 INFO mapred.JobClient:  map 68% reduce 19%
14/03/23 14:43:35 INFO mapred.JobClient:  map 69% reduce 19%
14/03/23 14:43:41 INFO mapred.JobClient:  map 69% reduce 20%
14/03/23 14:43:47 INFO mapred.JobClient:  map 70% reduce 20%
14/03/23 14:43:56 INFO mapred.JobClient:  map 70% reduce 21%
14/03/23 14:43:59 INFO mapred.JobClient:  map 71% reduce 21%
14/03/23 14:44:11 INFO mapred.JobClient:  map 72% reduce 21%
14/03/23 14:44:17 INFO mapred.JobClient:  map 72% reduce 22%
14/03/23 14:44:26 INFO mapred.JobClient:  map 73% reduce 23%
14/03/23 14:44:35 INFO mapred.JobClient:  map 74% reduce 23%
14/03/23 14:44:47 INFO mapred.JobClient:  map 75% reduce 23%
14/03/23 14:44:59 INFO mapred.JobClient:  map 76% reduce 23%
14/03/23 14:45:08 INFO mapred.JobClient:  map 77% reduce 23%
14/03/23 14:45:17 INFO mapred.JobClient:  map 78% reduce 23%
14/03/23 14:45:26 INFO mapred.JobClient:  map 79% reduce 23%
14/03/23 14:45:38 INFO mapred.JobClient:  map 80% reduce 23%
14/03/23 14:45:47 INFO mapred.JobClient:  map 81% reduce 23%
14/03/23 14:45:59 INFO mapred.JobClient:  map 82% reduce 24%
14/03/23 14:46:11 INFO mapred.JobClient:  map 83% reduce 24%
14/03/23 14:46:20 INFO mapred.JobClient:  map 83% reduce 25%
14/03/23 14:46:23 INFO mapred.JobClient:  map 84% reduce 25%
14/03/23 14:46:35 INFO mapred.JobClient:  map 85% reduce 26%
14/03/23 14:46:47 INFO mapred.JobClient:  map 86% reduce 26%
14/03/23 14:46:50 INFO mapred.JobClient:  map 86% reduce 27%
14/03/23 14:46:59 INFO mapred.JobClient:  map 87% reduce 27%
14/03/23 14:47:08 INFO mapred.JobClient:  map 88% reduce 27%
14/03/23 14:47:17 INFO mapred.JobClient:  map 89% reduce 27%
14/03/23 14:47:29 INFO mapred.JobClient:  map 90% reduce 27%
14/03/23 14:47:38 INFO mapred.JobClient:  map 91% reduce 27%
14/03/23 14:47:47 INFO mapred.JobClient:  map 92% reduce 27%
14/03/23 14:47:59 INFO mapred.JobClient:  map 93% reduce 27%
14/03/23 14:48:11 INFO mapred.JobClient:  map 94% reduce 28%
14/03/23 14:48:20 INFO mapred.JobClient:  map 95% reduce 28%
14/03/23 14:48:26 INFO mapred.JobClient:  map 95% reduce 29%
14/03/23 14:48:32 INFO mapred.JobClient:  map 96% reduce 29%
14/03/23 14:48:41 INFO mapred.JobClient:  map 96% reduce 30%
14/03/23 14:48:47 INFO mapred.JobClient:  map 97% reduce 30%
14/03/23 14:48:50 INFO mapred.JobClient:  map 97% reduce 31%
14/03/23 14:49:14 INFO mapred.JobClient:  map 98% reduce 31%
14/03/23 14:49:41 INFO mapred.JobClient:  map 99% reduce 31%
14/03/23 14:50:11 INFO mapred.JobClient:  map 99% reduce 32%
14/03/23 14:50:18 INFO mapred.JobClient:  map 100% reduce 32%
14/03/23 14:50:27 INFO mapred.JobClient:  map 100% reduce 49%
14/03/23 14:50:30 INFO mapred.JobClient:  map 100% reduce 68%
14/03/23 14:50:33 INFO mapred.JobClient:  map 100% reduce 71%
14/03/23 14:50:36 INFO mapred.JobClient:  map 100% reduce 76%
14/03/23 14:50:39 INFO mapred.JobClient:  map 100% reduce 80%
14/03/23 14:50:42 INFO mapred.JobClient:  map 100% reduce 84%
14/03/23 14:50:45 INFO mapred.JobClient:  map 100% reduce 89%
14/03/23 14:50:48 INFO mapred.JobClient:  map 100% reduce 91%
14/03/23 14:50:51 INFO mapred.JobClient:  map 100% reduce 97%
14/03/23 14:50:57 INFO mapred.JobClient:  map 100% reduce 100%
14/03/23 14:51:02 INFO mapred.JobClient: Job complete: job_201403231427_0001
14/03/23 14:51:02 INFO mapred.JobClient: Counters: 29
14/03/23 14:51:02 INFO mapred.JobClient:   Job Counters 
14/03/23 14:51:02 INFO mapred.JobClient:     Launched reduce tasks=2
14/03/23 14:51:02 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=13307629
14/03/23 14:51:02 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
14/03/23 14:51:02 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
14/03/23 14:51:02 INFO mapred.JobClient:     Launched map tasks=100
14/03/23 14:51:02 INFO mapred.JobClient:     Data-local map tasks=100
14/03/23 14:51:02 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=2167386
14/03/23 14:51:02 INFO mapred.JobClient:   File Output Format Counters 
14/03/23 14:51:02 INFO mapred.JobClient:     Bytes Written=293004675
14/03/23 14:51:02 INFO mapred.JobClient:   FileSystemCounters
14/03/23 14:51:02 INFO mapred.JobClient:     FILE_BYTES_READ=1009300145
14/03/23 14:51:02 INFO mapred.JobClient:     HDFS_BYTES_READ=3118758693
14/03/23 14:51:02 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=2021293728
14/03/23 14:51:02 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=293004675
14/03/23 14:51:02 INFO mapred.JobClient:   File Input Format Counters 
14/03/23 14:51:02 INFO mapred.JobClient:     Bytes Read=3068323935
14/03/23 14:51:02 INFO mapred.JobClient:   Map-Reduce Framework
14/03/23 14:51:02 INFO mapred.JobClient:     Map output materialized bytes=1009301933
14/03/23 14:51:02 INFO mapred.JobClient:     Map input records=14303375
14/03/23 14:51:02 INFO mapred.JobClient:     Reduce shuffle bytes=1000092673
14/03/23 14:51:02 INFO mapred.JobClient:     Spilled Records=204800
14/03/23 14:51:02 INFO mapred.JobClient:     Map output bytes=1008902950
14/03/23 14:51:02 INFO mapred.JobClient:     CPU time spent (ms)=12939060
14/03/23 14:51:02 INFO mapred.JobClient:     Total committed heap usage (bytes)=226725330944
14/03/23 14:51:02 INFO mapred.JobClient:     Combine input records=0
14/03/23 14:51:02 INFO mapred.JobClient:     SPLIT_RAW_BYTES=13200
14/03/23 14:51:02 INFO mapred.JobClient:     Reduce input records=102400
14/03/23 14:51:02 INFO mapred.JobClient:     Reduce input groups=1024
14/03/23 14:51:02 INFO mapred.JobClient:     Combine output records=0
14/03/23 14:51:02 INFO mapred.JobClient:     Physical memory (bytes) snapshot=115408035840
14/03/23 14:51:02 INFO mapred.JobClient:     Reduce output records=1024
14/03/23 14:51:02 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=1968714264576
14/03/23 14:51:02 INFO mapred.JobClient:     Map output records=102400
14/03/23 14:51:02 INFO driver.MahoutDriver: Program took 1286470 ms (Minutes: 21.441166666666668)

real	21m30.960s
user	0m10.853s
sys	0m2.701s
jmg3     13811  0.0  0.0 106084  1404 ?        Ss   14:51   0:00 bash -c ps aux | grep java
jmg3     13827  0.0  0.0 103232   844 ?        S    14:51   0:00 grep java
jmg3      4686  0.0  0.0  59072  3532 pts/0    S+   14:51   0:00 ssh -o ConnectTimeout=2 gpu-010 ps aux | grep java
jmg3      4692  0.0  0.0 106084  1396 ?        Ss   14:51   0:00 bash -c ps aux | grep java
jmg3      4708  0.0  0.0 103232   844 ?        S    14:51   0:00 grep java
java: no process killed
java: no process killed
jmg3     13874  0.0  0.0 106084  1404 ?        Ss   14:51   0:00 bash -c ps aux | grep java
jmg3     13890  0.0  0.0 103232   840 ?        S    14:51   0:00 grep java
jmg3      4770  0.0  0.0  59072  3528 pts/0    S+   14:51   0:00 ssh -o ConnectTimeout=2 gpu-010 ps aux | grep java
jmg3      4776  0.0  0.0 106084  1400 ?        Ss   14:51   0:00 bash -c ps aux | grep java
jmg3      4792  0.0  0.0 103232   840 ?        S    14:51   0:00 grep java
Setting path to /tmp/yiskylee
12 2 0 0 0 0 67108864 16
Max. num tasks = 12 mappers, 2 reducers
Mapper thread configs = GPU ( 0 ), CPU ( 0 )
Reducer thread configs = GPU ( 0 ), CPU ( 0 )
HDFS block size = 67108864
GPU/CPU Mults shouldn't matter



Here are the results:
export JAVA_HOME=/opt/apps/jdk/1.6
export HOME=/home/jmg3
export HADOOP_HOME=${HOME}/hadoop-1.0.3
export APARAPI_HOME=${HOME}/aparapi-read-only
export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true
export HADOOPCL_KERNEL_FOLDER=${HOME}/kernels
export HADOOP_LOG_DIR=/tmp/hadoop/logs
export HADOOP_APP_DIR=${HOME}/app
export CLASSPATH=${APARAPI_HOME}/com.amd.aparapi/dist/aparapi.jar:${HADOOP_HOME}/build/hadoop-core-1.0.4-SNAPSHOT.jar:${HOME}/hadoop-gpl-compression-read-only/build/hadoop-gpl-compression-0.2.0-dev.jar:${HADOOP_HOME}/build/hadoop-core-1.0.4-SNAPSHOT.jar:${HOME}/jfreechart-1.0.14.jar:.:${CLASSPATH}:/home/jmg3/mahout/math/target/mahout-math-0.9-SNAPSHOT.jar:/home/jmg3/mahout/integration/target/dependency/mahout-core-0.9-SNAPSHOT.jar
export HADOOP_CLASSPATH=${HOME}/hadoop-gpl-compression-read-only/build/hadoop-gpl-compression-0.2.0-dev.jar:${CLASSPATH}:${HADOOP_CLASSPATH}:/home/jmg3/mahout/math/target/mahout-math-0.9-SNAPSHOT.jar:/home/jmg3/mahout/integration/target/dependency/mahout-core-0.9-SNAPSHOT.jar
export JAVA_LIBRARY_PATH=${HOME}/lzo-install/lib:${JAVA_LIBRARY_PATH}

export SCIPY_HOME=${HOME}/scipy-install
export NUMPY_HOME=${HOME}/numpy-install
export SCIKIT_HOME=${HOME}/scikit-install
export PYTHONPATH=${SCIKIT_HOME}/lib64/python2.6/site-packages:${NUMPY_HOME}/lib64/python2.6/site-packages:${SCIPY_HOME}/lib64/python2.6/site-packages:${PYTHONPATH}

-----------------------------------------------------
  <name>mapred.job.tracker</name>
  <value>gpu-010.davinci.rice.edu:54311</value>
  <name>mapred.task.tracker.http.address</name>
  <value>0.0.0.0:50061</value>
  <name>mapred.reduce.parallel.copies</name><value>5</value>
  <name>tasktracker.http.threads</name><value>40</value>
  <name>mapred.map.tasks</name><value>24</value>
  <name>mapred.reduce.tasks</name><value>2</value>
  <name>mapred.tasktracker.map.tasks.maximum</name><value>12</value>
  <name>mapred.tasktracker.reduce.tasks.maximum</name><value>2</value>
  <name>mapred.map.tasks.speculative.execution</name>
  <value>true</value>
  <name>mapred.task.timeout</name><value>3600000</value>
  <!--name>mapred.task.timeout</name><value>1200000</value-->
  <name>mapred.child.java.opts</name><value>-Xms2G -Xmx16G -Dopencl.mapper.threadsPerGroup.gpu=0 -Dopencl.mapper.threadsPerGroup.cpu=0 -Dopencl.reducer.threadsPerGroup.gpu=0 -Dopencl.reducer.threadsPerGroup.cpu=0 -Dopencl.scheduler=BALANCED -Dopencl.vectorsToBuffer=32768</value>
  <name>mapred.map.tasks.speculative.execution</name><value>false</value>
  <name>mapred.reduce.tasks.speculative.execution</name><value>false</value>
  <name>mapreduce.map.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLBalanceScheduler</value>
  <!--name>mapreduce.map.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLML4Scheduler</value-->
  <name>mapreduce.reduce.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLBalanceScheduler</value>
  <!--name>mapreduce.reduce.ocl_scheduler</name><value>org.apache.hadoop.mapreduce.HadoopCLML4Scheduler</value-->
  <name>io.compression.codecs</name>
  <value>org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.BZip2Codec,org.apache.hadoop.io.compress.SnappyCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec</value>
  <name>io.sort.spill.percent</name>
  <value>0.7</value>
  <name>io.sort.spill.percent.hadoopcl</name>
  <value>0.6</value>
  <name>io.sort.spill.percent.hadoopcl.initial</name>
  <value>0.5</value>
  <name>io.sort.quickrestart</name>
  <value>0.4</value>
  <name>io.sort.maxrestart</name>
  <value>0.6</value>
  <name>io.sort.mb</name>
  <value>100</value>
  <name>min.num.spills.for.combine</name>
  <value>3</value>
  <name>opencl.global.buckets</name>
  <value>65536</value>
  <name>opencl.spill.chunk</name>
  <value>2</value>
<property><name>opencl.mapper.nkernels</name><value>1</value></property>
<property><name>opencl.mapper.ninputbuffers</name><value>2</value></property>
<property><name>opencl.mapper.noutputbuffers</name><value>4</value></property>
<property><name>opencl.mapper.prealloc.length.int</name><value>1966080</value></property>
<property><name>opencl.mapper.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.mapper.prealloc.length.double</name><value>1966080</value></property>
<property><name>opencl.mapper.inputBufferSize</name><value>65536</value></property>
<property><name>opencl.mapper.outputBufferSize</name><value>65536</value></property>
<property><name>opencl.mapper.val_ele_multiplier</name><value>15</value></property>
<!--property><name>opencl.mapper.compare</name><value>true</value></property-->
<property><name>opencl.combiner.nkernels</name><value>1</value></property>
<property><name>opencl.combiner.ninputbuffers</name><value>1</value></property>
<property><name>opencl.combiner.noutputbuffers</name><value>1</value></property>
<property><name>opencl.combiner.prealloc.length.int</name><value>7340032</value></property>
<property><name>opencl.combiner.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.combiner.prealloc.length.double</name><value>6291456</value></property>
<property><name>opencl.combiner.inputBufferSize</name><value>6000</value></property>
<property><name>opencl.combiner.outputBufferSize</name><value>16384</value></property>
<property><name>opencl.combiner.val_multiplier</name><value>60</value></property>
<property><name>opencl.combiner.val_ele_multiplier</name><value>20</value></property>
<property><name>opencl.reducer.nkernels</name><value>1</value></property>
<property><name>opencl.reducer.ninputbuffers</name><value>1</value></property>
<property><name>opencl.reducer.noutputbuffers</name><value>1</value></property>
<property><name>opencl.reducer.prealloc.length.int</name><value>16777216</value></property>
<property><name>opencl.reducer.prealloc.length.float</name><value>0</value></property>
<property><name>opencl.reducer.prealloc.length.double</name><value>16777216</value></property>
<property><name>opencl.reducer.inputBufferSize</name><value>65536</value></property>
<property><name>opencl.reducer.outputBufferSize</name><value>2048</value></property>
  <name>opencl.buffer.diagnostics</name>
  <value>false</value>
  <name>opencl.profiling</name>
  <value>false</value>
  <name>opencl.highlevel</name>
  <value>false</value>
  <name>opencl.recordings_folder</name>
  <value>/scratch/jmg3/hadoopcl-recordings/</value>
-----------------------------------------------------
  <name>dfs.safemode.threshold.pct</name>
  <value>0</value>
  <name>dfs.replication</name><value>3</value>
  <name>dfs.block.size</name><value>67108864</value>
  <name>dfs.datanode.handler.count</name><value>3</value>
  <name>dfs.namenode.handler.count</name><value>10</value>
  <name>dfs.datanode.max.xcievers</name><value>256</value>
-----------------------------------------------------
  <name>hadoop.tmp.dir</name>
  <value>/tmp/hadoop/hadoop-${user.name}</value>
  <name>fs.default.name</name>
  <value>hdfs://gpu-010.davinci.rice.edu:54313</value>
<name>io.compression.codecs</name>
<value>
</value>
    <name>io.compression.codec.lzo.class</name>
    <value>com.hadoop.compression.lzo.LzoCodec</value>
-----------------------------------------------------
gpu-009
-----------------------------------------------------
gpu-010
-----------------------------------------------------
Completed reconfiguring
Warning: $HADOOP_HOME is deprecated.

14/03/23 14:51:13 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = gpu-010.davinci.rice.edu/192.168.110.210
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 1.0.4-SNAPSHOT
STARTUP_MSG:   build =  -r ; compiled by 'jmg3' on Sun Mar 23 13:28:23 CDT 2014
************************************************************/
14/03/23 14:51:13 INFO util.GSet: VM type       = 64-bit
14/03/23 14:51:13 INFO util.GSet: 2% max memory = 17.77875 MB
14/03/23 14:51:13 INFO util.GSet: capacity      = 2^21 = 2097152 entries
14/03/23 14:51:13 INFO util.GSet: recommended=2097152, actual=2097152
14/03/23 14:51:13 INFO namenode.FSNamesystem: fsOwner=jmg3
14/03/23 14:51:13 INFO namenode.FSNamesystem: supergroup=supergroup
14/03/23 14:51:13 INFO namenode.FSNamesystem: isPermissionEnabled=true
14/03/23 14:51:13 INFO namenode.FSNamesystem: dfs.block.invalidate.limit=100
14/03/23 14:51:13 INFO namenode.FSNamesystem: isAccessTokenEnabled=false accessKeyUpdateInterval=0 min(s), accessTokenLifetime=0 min(s)
14/03/23 14:51:13 INFO namenode.NameNode: Caching file names occuring more than 10 times 
14/03/23 14:51:13 INFO common.Storage: Image file of size 110 saved in 0 seconds.
14/03/23 14:51:14 INFO common.Storage: Storage directory /tmp/hadoop/hadoop-jmg3/dfs/name has been successfully formatted.
14/03/23 14:51:14 INFO namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at gpu-010.davinci.rice.edu/192.168.110.210
************************************************************/
Completed namenode startup
Warning: $HADOOP_HOME is deprecated.

no jobtracker to stop
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: no tasktracker to stop
no namenode to stop
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: no datanode to stop
gpu-010: Warning: $HADOOP_HOME is deprecated.
gpu-010: 
gpu-010: no secondarynamenode to stop
Completed stop all
Warning: $HADOOP_HOME is deprecated.

starting namenode, logging to /tmp/hadoop/logs/hadoop-jmg3-namenode-gpu-010.davinci.rice.edu.out
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: starting datanode, logging to /tmp/hadoop/logs/hadoop-jmg3-datanode-gpu-009.davinci.rice.edu.out
gpu-010: Warning: $HADOOP_HOME is deprecated.
gpu-010: 
gpu-010: starting secondarynamenode, logging to /tmp/hadoop/logs/hadoop-jmg3-secondarynamenode-gpu-010.davinci.rice.edu.out
starting jobtracker, logging to /tmp/hadoop/logs/hadoop-jmg3-jobtracker-gpu-010.davinci.rice.edu.out
gpu-009: Warning: $HADOOP_HOME is deprecated.
gpu-009: 
gpu-009: starting tasktracker, logging to /tmp/hadoop/logs/hadoop-jmg3-tasktracker-gpu-009.davinci.rice.edu.out
Completed start all
Warning: $HADOOP_HOME is deprecated.

Warning: $HADOOP_HOME is deprecated.

Warning: $HADOOP_HOME is deprecated.

MAHOUT_LOCAL is not set; adding HADOOP_CONF_DIR to classpath.
Warning: $HADOOP_HOME is deprecated.

Running on hadoop, using /home/jmg3/hadoop-1.0.3/bin/hadoop and HADOOP_CONF_DIR=/home/jmg3/hadoop-1.0.3/conf
MAHOUT-JOB: /home/jmg3/mahout/examples/target/mahout-examples-0.9-SNAPSHOT-job.jar
Warning: $HADOOP_HOME is deprecated.

14/03/23 14:53:42 INFO common.AbstractJob: Command line arguments: {--clusters=[clusters], --convergenceDelta=[0.5], --distanceMeasure=[org.apache.mahout.common.distance.CosineDistanceMeasure], --endPhase=[2147483647], --input=[input], --maxIter=[1], --method=[mapreduce], --output=[output], --overwrite=null, --startPhase=[0], --tempDir=[temp]}
14/03/23 14:53:42 INFO kmeans.KMeansDriver: Input: input Clusters In: clusters Out: output Distance: org.apache.mahout.common.distance.CosineDistanceMeasure
14/03/23 14:53:42 INFO kmeans.KMeansDriver: convergence: 0.5 max Iterations: 1
14/03/23 14:53:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/03/23 14:53:42 INFO compress.CodecPool: Got brand-new decompressor
Cluster Iterator running iteration 1/1 over priorPath: output/clusters-0
Sending 0 to HDFS took 98 ms
14/03/23 14:54:13 INFO input.FileInputFormat: Total input paths to process : 100
14/03/23 14:54:17 INFO mapred.JobClient: Running job: job_201403231451_0001
14/03/23 14:54:18 INFO mapred.JobClient:  map 0% reduce 0%
14/03/23 14:54:49 INFO mapred.JobClient:  map 1% reduce 0%
14/03/23 14:55:01 INFO mapred.JobClient:  map 2% reduce 0%
14/03/23 14:55:13 INFO mapred.JobClient:  map 3% reduce 0%
14/03/23 14:55:22 INFO mapred.JobClient:  map 4% reduce 0%
14/03/23 14:55:37 INFO mapred.JobClient:  map 5% reduce 0%
14/03/23 14:55:49 INFO mapred.JobClient:  map 6% reduce 0%
14/03/23 14:55:58 INFO mapred.JobClient:  map 7% reduce 0%
14/03/23 14:56:10 INFO mapred.JobClient:  map 8% reduce 0%
14/03/23 14:56:19 INFO mapred.JobClient:  map 9% reduce 0%
14/03/23 14:56:31 INFO mapred.JobClient:  map 10% reduce 0%
14/03/23 14:56:40 INFO mapred.JobClient:  map 11% reduce 0%
14/03/23 14:56:58 INFO mapred.JobClient:  map 12% reduce 0%
14/03/23 14:57:01 INFO mapred.JobClient:  map 12% reduce 2%
14/03/23 14:57:04 INFO mapred.JobClient:  map 12% reduce 3%
14/03/23 14:57:13 INFO mapred.JobClient:  map 13% reduce 3%
14/03/23 14:57:25 INFO mapred.JobClient:  map 14% reduce 3%
14/03/23 14:57:37 INFO mapred.JobClient:  map 15% reduce 3%
14/03/23 14:57:46 INFO mapred.JobClient:  map 16% reduce 3%
14/03/23 14:57:58 INFO mapred.JobClient:  map 17% reduce 3%
14/03/23 14:58:07 INFO mapred.JobClient:  map 18% reduce 3%
14/03/23 14:58:19 INFO mapred.JobClient:  map 19% reduce 3%
14/03/23 14:58:31 INFO mapred.JobClient:  map 20% reduce 3%
14/03/23 14:58:40 INFO mapred.JobClient:  map 21% reduce 3%
14/03/23 14:58:52 INFO mapred.JobClient:  map 22% reduce 3%
14/03/23 14:59:01 INFO mapred.JobClient:  map 23% reduce 3%
14/03/23 14:59:13 INFO mapred.JobClient:  map 23% reduce 4%
14/03/23 14:59:16 INFO mapred.JobClient:  map 23% reduce 6%
14/03/23 14:59:19 INFO mapred.JobClient:  map 24% reduce 6%
14/03/23 14:59:22 INFO mapred.JobClient:  map 24% reduce 7%
14/03/23 14:59:34 INFO mapred.JobClient:  map 25% reduce 7%
14/03/23 14:59:43 INFO mapred.JobClient:  map 26% reduce 7%
14/03/23 14:59:55 INFO mapred.JobClient:  map 27% reduce 7%
14/03/23 15:00:04 INFO mapred.JobClient:  map 28% reduce 7%
14/03/23 15:00:16 INFO mapred.JobClient:  map 29% reduce 7%
14/03/23 15:00:25 INFO mapred.JobClient:  map 30% reduce 7%
14/03/23 15:00:37 INFO mapred.JobClient:  map 31% reduce 7%
14/03/23 15:00:46 INFO mapred.JobClient:  map 32% reduce 7%
14/03/23 15:00:58 INFO mapred.JobClient:  map 33% reduce 7%
14/03/23 15:01:07 INFO mapred.JobClient:  map 34% reduce 7%
14/03/23 15:01:19 INFO mapred.JobClient:  map 35% reduce 7%
14/03/23 15:01:25 INFO mapred.JobClient:  map 35% reduce 8%
14/03/23 15:01:35 INFO mapred.JobClient:  map 36% reduce 9%
14/03/23 15:01:41 INFO mapred.JobClient:  map 36% reduce 10%
14/03/23 15:01:47 INFO mapred.JobClient:  map 36% reduce 11%
14/03/23 15:01:50 INFO mapred.JobClient:  map 37% reduce 11%
14/03/23 15:02:02 INFO mapred.JobClient:  map 38% reduce 11%
14/03/23 15:02:11 INFO mapred.JobClient:  map 39% reduce 11%
14/03/23 15:02:23 INFO mapred.JobClient:  map 40% reduce 11%
14/03/23 15:02:32 INFO mapred.JobClient:  map 41% reduce 11%
14/03/23 15:02:44 INFO mapred.JobClient:  map 42% reduce 11%
14/03/23 15:02:53 INFO mapred.JobClient:  map 43% reduce 11%
14/03/23 15:03:05 INFO mapred.JobClient:  map 44% reduce 11%
14/03/23 15:03:14 INFO mapred.JobClient:  map 45% reduce 11%
14/03/23 15:03:26 INFO mapred.JobClient:  map 46% reduce 11%
14/03/23 15:03:38 INFO mapred.JobClient:  map 47% reduce 11%
14/03/23 15:03:41 INFO mapred.JobClient:  map 47% reduce 12%
14/03/23 15:03:50 INFO mapred.JobClient:  map 47% reduce 13%
14/03/23 15:03:53 INFO mapred.JobClient:  map 48% reduce 13%
14/03/23 15:03:56 INFO mapred.JobClient:  map 48% reduce 14%
14/03/23 15:04:05 INFO mapred.JobClient:  map 49% reduce 14%
14/03/23 15:04:11 INFO mapred.JobClient:  map 49% reduce 15%
14/03/23 15:04:17 INFO mapred.JobClient:  map 50% reduce 15%
14/03/23 15:04:29 INFO mapred.JobClient:  map 51% reduce 15%
14/03/23 15:04:38 INFO mapred.JobClient:  map 52% reduce 15%
14/03/23 15:04:50 INFO mapred.JobClient:  map 53% reduce 15%
14/03/23 15:04:59 INFO mapred.JobClient:  map 54% reduce 15%
14/03/23 15:05:08 INFO mapred.JobClient:  map 55% reduce 15%
14/03/23 15:05:20 INFO mapred.JobClient:  map 56% reduce 15%
14/03/23 15:05:29 INFO mapred.JobClient:  map 57% reduce 15%
14/03/23 15:05:41 INFO mapred.JobClient:  map 58% reduce 15%
14/03/23 15:05:50 INFO mapred.JobClient:  map 58% reduce 16%
14/03/23 15:05:53 INFO mapred.JobClient:  map 59% reduce 16%
14/03/23 15:05:56 INFO mapred.JobClient:  map 59% reduce 17%
14/03/23 15:06:08 INFO mapred.JobClient:  map 60% reduce 17%
14/03/23 15:06:11 INFO mapred.JobClient:  map 60% reduce 18%
14/03/23 15:06:17 INFO mapred.JobClient:  map 60% reduce 19%
14/03/23 15:06:20 INFO mapred.JobClient:  map 61% reduce 19%
14/03/23 15:06:32 INFO mapred.JobClient:  map 62% reduce 19%
14/03/23 15:06:44 INFO mapred.JobClient:  map 63% reduce 19%
14/03/23 15:06:53 INFO mapred.JobClient:  map 64% reduce 19%
14/03/23 15:07:02 INFO mapred.JobClient:  map 65% reduce 19%
14/03/23 15:07:14 INFO mapred.JobClient:  map 66% reduce 19%
14/03/23 15:07:23 INFO mapred.JobClient:  map 67% reduce 19%
14/03/23 15:07:32 INFO mapred.JobClient:  map 68% reduce 19%
14/03/23 15:07:44 INFO mapred.JobClient:  map 69% reduce 19%
14/03/23 15:07:56 INFO mapred.JobClient:  map 70% reduce 20%
14/03/23 15:08:08 INFO mapred.JobClient:  map 71% reduce 20%
14/03/23 15:08:11 INFO mapred.JobClient:  map 71% reduce 21%
14/03/23 15:08:20 INFO mapred.JobClient:  map 71% reduce 22%
14/03/23 15:08:23 INFO mapred.JobClient:  map 72% reduce 22%
14/03/23 15:08:35 INFO mapred.JobClient:  map 73% reduce 22%
14/03/23 15:08:41 INFO mapred.JobClient:  map 73% reduce 23%
14/03/23 15:08:47 INFO mapred.JobClient:  map 74% reduce 23%
14/03/23 15:08:56 INFO mapred.JobClient:  map 75% reduce 23%
14/03/23 15:09:08 INFO mapred.JobClient:  map 76% reduce 23%
14/03/23 15:09:17 INFO mapred.JobClient:  map 77% reduce 23%
14/03/23 15:09:26 INFO mapred.JobClient:  map 78% reduce 23%
14/03/23 15:09:35 INFO mapred.JobClient:  map 79% reduce 23%
14/03/23 15:09:47 INFO mapred.JobClient:  map 80% reduce 23%
14/03/23 15:09:56 INFO mapred.JobClient:  map 81% reduce 23%
14/03/23 15:10:08 INFO mapred.JobClient:  map 82% reduce 23%
14/03/23 15:10:11 INFO mapred.JobClient:  map 82% reduce 24%
14/03/23 15:10:20 INFO mapred.JobClient:  map 83% reduce 24%
14/03/23 15:10:26 INFO mapred.JobClient:  map 83% reduce 25%
14/03/23 15:10:35 INFO mapred.JobClient:  map 84% reduce 25%
14/03/23 15:10:41 INFO mapred.JobClient:  map 84% reduce 26%
14/03/23 15:10:47 INFO mapred.JobClient:  map 85% reduce 26%
14/03/23 15:10:56 INFO mapred.JobClient:  map 85% reduce 27%
14/03/23 15:10:59 INFO mapred.JobClient:  map 86% reduce 27%
14/03/23 15:11:08 INFO mapred.JobClient:  map 87% reduce 27%
14/03/23 15:11:20 INFO mapred.JobClient:  map 88% reduce 27%
14/03/23 15:11:28 INFO mapred.JobClient:  map 89% reduce 27%
14/03/23 15:11:37 INFO mapred.JobClient:  map 90% reduce 27%
14/03/23 15:11:49 INFO mapred.JobClient:  map 91% reduce 27%
14/03/23 15:11:58 INFO mapred.JobClient:  map 92% reduce 27%
14/03/23 15:12:11 INFO mapred.JobClient:  map 93% reduce 27%
14/03/23 15:12:20 INFO mapred.JobClient:  map 94% reduce 27%
14/03/23 15:12:32 INFO mapred.JobClient:  map 95% reduce 27%
14/03/23 15:12:35 INFO mapred.JobClient:  map 95% reduce 29%
14/03/23 15:12:44 INFO mapred.JobClient:  map 96% reduce 29%
14/03/23 15:12:50 INFO mapred.JobClient:  map 96% reduce 30%
14/03/23 15:13:02 INFO mapred.JobClient:  map 97% reduce 30%
14/03/23 15:13:05 INFO mapred.JobClient:  map 97% reduce 31%
14/03/23 15:13:26 INFO mapred.JobClient:  map 98% reduce 31%
14/03/23 15:13:53 INFO mapred.JobClient:  map 99% reduce 31%
14/03/23 15:14:23 INFO mapred.JobClient:  map 100% reduce 31%
14/03/23 15:14:29 INFO mapred.JobClient:  map 100% reduce 32%
14/03/23 15:14:35 INFO mapred.JobClient:  map 100% reduce 67%
14/03/23 15:14:38 INFO mapred.JobClient:  map 100% reduce 71%
14/03/23 15:14:41 INFO mapred.JobClient:  map 100% reduce 75%
14/03/23 15:14:44 INFO mapred.JobClient:  map 100% reduce 80%
14/03/23 15:14:47 INFO mapred.JobClient:  map 100% reduce 84%
14/03/23 15:14:50 INFO mapred.JobClient:  map 100% reduce 87%
14/03/23 15:14:53 INFO mapred.JobClient:  map 100% reduce 90%
14/03/23 15:14:56 INFO mapred.JobClient:  map 100% reduce 92%
14/03/23 15:14:59 INFO mapred.JobClient:  map 100% reduce 97%
14/03/23 15:15:08 INFO mapred.JobClient:  map 100% reduce 100%
14/03/23 15:15:13 INFO mapred.JobClient: Job complete: job_201403231451_0001
14/03/23 15:15:13 INFO mapred.JobClient: Counters: 29
14/03/23 15:15:13 INFO mapred.JobClient:   Job Counters 
14/03/23 15:15:13 INFO mapred.JobClient:     Launched reduce tasks=2
14/03/23 15:15:13 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=13378036
14/03/23 15:15:13 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
14/03/23 15:15:13 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
14/03/23 15:15:13 INFO mapred.JobClient:     Launched map tasks=100
14/03/23 15:15:13 INFO mapred.JobClient:     Data-local map tasks=100
14/03/23 15:15:13 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=2182468
14/03/23 15:15:13 INFO mapred.JobClient:   File Output Format Counters 
14/03/23 15:15:13 INFO mapred.JobClient:     Bytes Written=293004675
14/03/23 15:15:13 INFO mapred.JobClient:   FileSystemCounters
14/03/23 15:15:13 INFO mapred.JobClient:     FILE_BYTES_READ=1009300145
14/03/23 15:15:13 INFO mapred.JobClient:     HDFS_BYTES_READ=3118758693
14/03/23 15:15:13 INFO mapred.JobClient:     FILE_BYTES_WRITTEN=2021293728
14/03/23 15:15:13 INFO mapred.JobClient:     HDFS_BYTES_WRITTEN=293004675
14/03/23 15:15:13 INFO mapred.JobClient:   File Input Format Counters 
14/03/23 15:15:13 INFO mapred.JobClient:     Bytes Read=3068323935
14/03/23 15:15:13 INFO mapred.JobClient:   Map-Reduce Framework
14/03/23 15:15:13 INFO mapred.JobClient:     Map output materialized bytes=1009301933
14/03/23 15:15:13 INFO mapred.JobClient:     Map input records=14303375
14/03/23 15:15:13 INFO mapred.JobClient:     Reduce shuffle bytes=1000218322
14/03/23 15:15:13 INFO mapred.JobClient:     Spilled Records=204800
14/03/23 15:15:13 INFO mapred.JobClient:     Map output bytes=1008902950
14/03/23 15:15:13 INFO mapred.JobClient:     CPU time spent (ms)=13032740
14/03/23 15:15:13 INFO mapred.JobClient:     Total committed heap usage (bytes)=227527753728
14/03/23 15:15:13 INFO mapred.JobClient:     Combine input records=0
14/03/23 15:15:13 INFO mapred.JobClient:     SPLIT_RAW_BYTES=13200
14/03/23 15:15:13 INFO mapred.JobClient:     Reduce input records=102400
14/03/23 15:15:13 INFO mapred.JobClient:     Reduce input groups=1024
14/03/23 15:15:13 INFO mapred.JobClient:     Combine output records=0
14/03/23 15:15:13 INFO mapred.JobClient:     Physical memory (bytes) snapshot=115755421696
14/03/23 15:15:13 INFO mapred.JobClient:     Reduce output records=1024
14/03/23 15:15:13 INFO mapred.JobClient:     Virtual memory (bytes) snapshot=1968914538496
14/03/23 15:15:13 INFO mapred.JobClient:     Map output records=102400
14/03/23 15:15:13 INFO driver.MahoutDriver: Program took 1290878 ms (Minutes: 21.514633333333332)

real	21m35.317s
user	0m10.562s
sys	0m2.779s
